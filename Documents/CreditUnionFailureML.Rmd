---
title: "Predicting Credit Union Failure - A Machine Learning Approach"
author: "1867514 "
date: '`r format(Sys.time(), "%d %B %Y")`'
css: styles.css
output:
  html_document:
    code_folding: hide
---


```{r, include =FALSE}
library(tidyverse)
library(data.table)
library(stargazer)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(ggcorrplot)
library(parsnip)
library(ranger)
library(tictoc)
library(imbalance)
library(cvAUC)
library(ROSE)
library(gridExtra)
library(mice)
library(xgboost)
library(kknn)
library(discrim)
library(kernlab)
library(ISLR)
library(MASS)
library(e1071)
library(gmodels)
library(neuralnet)
library(NeuralNetTools)
library(caret)
library(formattable)
library(kableExtra)
library(grid)
library(graphics)
library(randomForest)
basePath = file.path("C:/Users/328576/source/repos/CreditUnionFailureML")
source(file.path(basePath, "Scripts/MachineLearningFunctions.R"))

options(scipen = 999)
options(digits = 2)
knitr::opts_chunk$set(echo=FALSE)
options(width = 100)
```

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}
# source data from files
dataRaw = data.table:: fread(file.path(basePath, "Data/CreditUnionMLData.csv"), stringsAsFactors = FALSE)
#select columns to keep
cols = colnames(dataRaw)[which(!grepl(pattern = "failure", x = colnames(dataRaw), ignore.case = TRUE))]
#prepare data 
dataClean = dataRaw[, match(cols, colnames(dataRaw)), with = FALSE]
dataClean = dataClean[, `:=`(CapitalRatio = CY_30E)][,-"CY_30E"]
```

<!-- <div class="logos"><img src="logo.png" width="220px" align="right"></div> -->

<hr /> 

**Masters Programmes: Assignment Cover Sheet**

<hr /> 

**Submitted by: 1867514** 

**Date Sent: 22 May 2020** 

**Module Title: Big Data Analytics** 

**Module Code: IB9N0L**

**Date/Year of Module: Spring 2020** 

**Submission Deadline: 27 May 2020** 

**Word Count: 3500 excluding appendix and references** 

**Number of Pages: 10** 


<hr />

<br />

'_I declare that this work is entirely my own in accordance with the University's Regulation 11 and the
WBS guidelines on plagiarism and collusion. All external references and sources are clearly
acknowledged and identified within the contents.
No substantial part(s) of the work submitted here has also been submitted by me in other
assessments for accredited courses of study, and I acknowledge that if this has been done it may
result in me being reported for self-plagiarism and an appropriate reduction in marks may be made
when marking this piece of work._'

<br />

<hr />

<br />

<P style="page-break-before: always">

This paper examines the application of a suite of machine learning methods looking to predict credit union failure in the UK. The aim is to support the supervisory review process by understanding the benefits and limitations of machine learning as an early warning system for identifying credit union failures. A set of indicators on a firms financial health combined with macroeconomic variables such as growth and inflation are used as the feature set in the analysis. The report is divided as follows: (i) Chapter 1 introduces the classification problem and the feature set. (ii) Chapter 2 explores the pre-processing steps in preparing the data for training (model fit), validation (calibration) and testing (evaluation). (iii) Chapter 3 analyses the performance of nine classification algorithms, which includes, linear discriminant analysis, neural networks, naive Bayes, random forests and support vector machines. Performance is set out according to some criterion applied to the testing data, hence out-of-sample evaluation. (iv) Chapter 4 takes the best performing subset of these methods forward to perform cross validation, whilst covering important concepts such as the bias-variance trade-off, optimal model complexity and scalability.(v) Chapter 5 explores feature importance and (iv) Chapter 6 provides a conclusion and concise model summary.

# Introduction {#intro}

Credit union failures are not uncommon. There have been 78 occurrences since 2002 as recorded in the data set utilised in this paper, equating to an average of 5/year. Credit union business models are inherently risky, with some offering products and services to a homogenous group of clients or having large exposures relative to their overall balance sheet size. This could make it hard for them to diversify their risks compared with larger banks. A credit union failure is where an institution is referred to the Financial Services Compensation Scheme (FSCS) for depositor pay-out. The data consists of a set of firm level financial indicators compiled from regulatory returns and macroeconomic variables covering the UK for the period 2002 to 2018 [@coen2018]. 

The feature set includes firm measures of capital, liquidity, non-performing loans and balance sheet size. This is combined with macroeconomic indicators on unemployment (regionally), inflation (CPI) and economic growth (GDP, Quarterly). The response variable is a credit union `DEFAULT`, which is a Boolean indicator that takes the value `1` if the credit union defaults and `0` otherwise. Table 1 shows the summary statistics for the predictors below.

<br /> 

```{r, results='asis'}
variables<-c("roa", "pctarrears", "loandeposit", "simplecapitalratio",
             "provlns", "logassets", "unsecassets", "CapitalRatio", "arrears312", "arrearsg12",
             "CPI", "GDP.Growth.Quarter.Last.Year", "Base.Rate", "Regunemployment")

y <- dataClean[, variables, with = FALSE]

colnames(y)<- c("Return on Assets (%)","Arrears rate (%)", "Loan-deposit ratio (%)", "Risk-adjusted capital ratio (%)", 
               "Provisions-loans ratio (%)", paste0("Log assets(", "\u00A3", ")"), "Unsecured loans to assets (%)", "Capital ratio (%)", "Arrears < 12 Months (%)", "Arrears > 12 Months (%)", "Inflation (%, CPI)", "GDP Growth (Quarterly, %)",  "Bank Rate (%)", "Regional Unemployment Rate (%)")

stargazer:: stargazer(y,type="html", digits=1, title = "Table 1: Summary Statistics of Credit Union Data", 
                      column.sep.width = "1000pt", font.size = "tiny", align = TRUE, 
                      omit.summary.stat = c("max", "min")
)


```

<br /> 

The dataset is heavily imbalanced, with ```r nrow(dataClean[which(dataClean$DEFAULT != 0)])``` `DEFAULT` occurrences in the data and ```r nrow(dataClean[which(dataClean$DEFAULT != 1)])``` active credit union observations at the date of reporting. The imbalanced nature of the dataset will be explored more in [Chapter 3](#modelsetup).  

In order to develop a better understanding of the data, a boxplot of each of the predictors are compared against `DEFAULT` rates are shown in Chart 1. 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

variables<-c("DEFAULT", "roa", "pctarrears", "loandeposit", "simplecapitalratio",
             "provlns", "logassets", "unsecassets", "CapitalRatio", "arrears312", "arrearsg12",
             "CPI", "GDP.Growth.Quarter.Last.Year", "Base.Rate", "Regunemployment")

DataML <- dataClean[, variables, with = FALSE]
DataML <- na.omit(DataML)
DataML <- PrepForRecipe(as.data.frame(DataML))

dataPlot <- tidyr:: gather(DataML, key = Feature, value = Position, -DEFAULT) %>% dplyr:: filter(Feature %in% c("logassets", "roa", "loandeposit", "pctarrears", "Regunemployment", "unsecassets"))

#box plot

p <- ggplot2:: ggplot(dataPlot, ggplot2:: aes(x = DEFAULT, y = Position, fill = DEFAULT)) +
  ggplot2:: geom_boxplot() +
  ggplot2:: labs(title = NULL, subtitle = "Chart 1: Box plot of predictors against Defaults",
                 y=NULL, x=NULL)  +
  ggplot2:: theme_minimal() +
  ggplot2:: scale_fill_manual(values = rep(unname(boe_cols), 100)) + 
  ggplot2:: coord_flip() +
  ggplot2:: facet_wrap(~Feature, scales = "free_x") 

p

```

Chart 1 provides some useful insights into the relationship between `DEFAULT` and the predictors in the data. It is clear that `DEFAULT` rates amongst credit unions are generally higher when total assets and return on assets (`roa`) are lower. Similarly, `DEFAULT` rates appear to be lower amongst entities with overall lower loan to deposit ratios, percentage of loans in arrears, unsecured assets and regional unemployment. These are all intuitive results, for example it is expected that the firm size to be inversely correlated with `DEFAULT` rates, since larger credit unions are likely to have a more diverse balance sheet and can withstand economic shocks. To understand how these variables interact with one another, Chart 2 shows a correlation plot of the features. 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

corr <- round(cor(dplyr:: select(DataML, -DEFAULT)), 1)

# Plot
ggcorrplot:: ggcorrplot(corr, hc.order = TRUE, 
                        type = "lower", 
                        lab = TRUE, 
                        lab_size = 3, 
                        method="circle", 
                        colors = c("#CAC0B6","white", "#006663"), 
                        title="Chart 2: Correlogram of predictors", 
                        ggtheme=theme_bw)

```

Chart 2 suggests there is some correlation between features, most notably unsecured assets and the loan to deposit ratio, percentage loans in arrears and loan provisions and some pronounced correlation amongst the macroeconomic indicators. A consideration here is to remove features that are highly correlated, particularly to limit issues with dimensionality and scalability of the algorithms; however this is not applied here.

# Data Preparation {#modelsetup}

In this chapter the data is prepared, which includes pre-processing and 'rebalancing' the `DEFAULT` class. First the training set and test set are generated on the basis of random sampling, with the split set to be 70% training and 30% test (`p = 0.7`). A relatively large proportion of the data is kept back for testing, which is due to the class imbalances, requiring a larger portion of the data for evaluation purposes. The steps taken to prepare the training and test sets for the model are shown below, with the results displayed in Table 2. 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

variables<-c("Year", "DEFAULT", "roa", "pctarrears", "loandeposit", "simplecapitalratio",
             "provlns", "logassets", "unsecassets", "CapitalRatio", "arrears312", "arrearsg12",
             "CPI", "GDP.Growth.Quarter.Last.Year", "Base.Rate", "Regunemployment")

DataML <- dataClean[, variables, with = FALSE]
DataML <- na.omit(DataML)

# prepare data for ML analysis
DataML <- PrepForRecipe(as.data.frame(DataML))
nrow_DataML <- tibble:: tibble(`Before Split` = nrow(DataML))

set.seed(12345)
train_test_split <- rsample:: initial_split(DataML, prop = 0.7)
train <- rsample:: training(train_test_split) # create training set 
test <- rsample:: testing(train_test_split) # create test set 

nrow_train_data <- tibble:: tibble(`Training set` = nrow(train))
nrow_test_data <- tibble:: tibble(`Test set` = nrow(test))

table <- cbind(nrow_DataML, nrow_train_data, nrow_test_data)
kableExtra:: kable(table, caption = "Training and test sets") %>% 
  kableExtra:: kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "left")
```

As mentioned, the credit union data is heavily imbalanced. To rebalance the data, up-sampling is applied in addition to data pre-processing, which replicates rows of a data set to make the occurrence of levels in a specific class more balanced. Down-sampling was also considered, whereby the majority class is sampled and removed from the data. There are more sophisticated rebalancing methods, such as generating synthetic data from the minority class (SMOTE) to increase its cardinality, which will be explored in [Chapter 4](#cv). 

Pre-processing refers to the transformations applied to our data before running the models. The features undergo the following steps;

* `step_center` - normalizes (numeric) variables to have mean zero
* `step_scale` - normalizes variables to have standard deviation one
* `step_dummy` - creates dummy variables from factor levels
* `step_upsample` - rebalances the data by up-sampling the `DEFAULT` class observations.

The implementation of this can be found in the [Annex](#annex) with the results shown in Table 3 [@rebecca2019].

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval=TRUE}

#---------------------------------------------------------------------------------------
# Create 'recipe' for pre-processing input data, and apply to one example split of data --------

simple_recipe <- function(dataset){
  recipes:: recipe(DEFAULT ~ ., data = dataset) %>%
    recipes:: step_center(all_numeric()) %>%
    recipes:: step_scale(all_numeric()) %>%
    recipes:: step_dummy(all_nominal(),- DEFAULT) %>%  
    recipes:: step_upsample(DEFAULT, ratio = 0.5) # this step rebalances the data 
}

train_recipe <- recipes:: prep(simple_recipe(train), training = train, retain = TRUE) # prepare data model, apply processing steps
train_data <- train_recipe %>% juice  # generate training set 
test_data <- recipes:: bake(train_recipe, new_data = test) # generate test set 

table <- rbind(c(CountDefaultsTrain = sum(train_data$DEFAULT == 1), CountNonDefaultsTrain = sum(train_data$DEFAULT == 0)), 
               c(CountDefaultsTest = sum(test_data$DEFAULT == 1), CountNonDefaultsTest = sum(test_data$DEFAULT == 0)))

rownames(table) <- c("training data", "test data")


```

```{r, results='asis', include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

stargazer:: stargazer(as.data.frame(train_data),type="html", digits=1, title = "Table 3: Summary Statistics of prep processed training data", 
                      column.sep.width = "1000pt", font.size = "tiny", align = TRUE
)

```

Input pre-processing is an important aspect, which may crucially affect a models predictive performance. Several techniques, namely those that combine inputs such as artificial neural networks are highly sensitive to different features. This is addressed via feature standardisation (as above), which allows all features to be compared easily and applied to different model types [@chakraborty2017]. The data is now ready to apply the learning algorithms, which is explored in [Chapter 3](#modelfit). 

# Model Fitting {#modelfit}

## Measuring Performance 

In this chapter a set of popular models within the machine learning toolbox are considered. The objective here is to fit an array of models the data (training set) and evaluate each models performance using a set of accuracy measures. The models considered include logistic regression (baseline), naive Bayes, k-nearest neighbours (KNN), tree algorithms, artificial neural networks (ANN), linear discriminant analysis (LDA) and support vector machines (SVM).

Model performance is considered on the basis that a there is an overall relative preference between a Type I error (misclassifying a default, `DEFAULT = 1`) and a Type II error (misclassifying a healthy firm, `DEFAULT = 0`) [@coen2018]. 

Five accuracy measures are used to assess model performance [@brett2019];

* **Accuracy** (poor measure for imbalanced data) - $(TP+TN)/(P + N)$ 
* **Balanced accuracy** - $(TP/P+TN/N)/2$ 
* **Sensitivity** (true positive rate) - $TP/(TP + FN)$
* **Specificity** (true negative rate) - $TN/(TN + FP)$
* **Receiver Operating Characteristic (ROC)** - sensitivity vs (1 − specificity)

Here there is a trade off from a supervisory perspective in which supervisors are concerned with the elements of the confusion matrix that are of high importance. Firstly, missing credit union defaults, otherwise known as false negatives. Secondly, raising false alarms on viable credit unions and wasting supervisory resources. The first is captured in the **Specificity** of the model, a high **Specificity** means the model is correctly identifying `DEFAULT` classes. The second is captured in the **Sensitivity**, with a lower **Sensitivity** meaning there are a lot of false flags in the predictions. The **ROC** curve is a useful performance measure as the inherent computation shows the trade-off between the benefits. These measures will be considered throughout this analysis to understand how the models are performing relative to one-another. 

## Baseline Logistic Model

First a logistic model is applied to the data to act as a baseline for performance evaluation. Logistic models are used to predict the probability that an observation falls into one of two categories; `DEFAULT = 1` or `DEFAULT = 0` based on a set of predictors (features). The instantiation of the logistic model is given in the [Annex](#annex). 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE}

#---------------------------------------------------------------------------------------
# fit logistic model using parsnip logistic_reg function, fit using training data 
logisticModel <- parsnip:: logistic_reg(mode = "classification", penalty = "glm") %>% 
  parsnip:: set_engine("glm") %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

#obtain model results using function (see annex for details) applying to the test data 
logisticModelResults <- modelResults(logisticModel, testData = test_data, "logisticModel")


```

<br /> 

Table 3.1 shows the model performance statistics. Note the accuracy is 91%; however this is not a good measure due to the class imbalances in the data. The balanced accuracy is 71%, whilst the specificity is only around 50%. The interpretation of this is that the logistic model is predicting around half of the `DEFAULT` cases correctly, whilst there are a number of false positives. Remember a supervisor has a relative preference of specificity, whilst minimising the number of false flags. All accuracy measures are computed using the result’s from the confusion matrix in Table 3.1

<br /> 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

#---------------------------------------------------------------------------------------
# Output models results 
results <- dplyr:: filter(logisticModelResults$modelStats, .metric %in% c("accuracy", "sens", "spec", "bal_accuracy", "precision", "roc_auc")) %>%  dplyr:: mutate(.estimate = formattable:: color_bar("#CAC0B6")(.estimate))

cm = logisticModelResults$confMatrix
dt = data.frame(t(cm$table)) %>% tidyr:: spread(key = Prediction, value = Freq) %>%   
  dplyr:: mutate(`0` = formattable:: color_tile("lightpink", "lightgreen")(`0`), 
                 `1` = formattable:: color_tile("lightgreen", "lightpink")(`1`))

kableExtra:: kable(results, able.attr = "style = \"color: black;\"", caption = "Model performance for logistic regression", escape = F) %>% 
  kableExtra:: kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = TRUE, position = "float_left")

kableExtra:: kable(dt, able.attr = "style = \"color: black;\"", escape = F, caption = "Confusion matrix") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "left") %>%
  kableExtra:: add_header_above(c(" ", "Prediction" = 2))


```

<br /> 

Chart 3 shows the ROC curve for the logistic regression baseline model, which shows the trade-off between sensitivity and specificity. Classifiers that give curves closer to the top-left corner indicate a better performance [@brett2019].

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE, out.height= "50%", out.width="80%"}

#---------------------------------------------------------------------------------------
# Output ROC results 
logisticModelResults$modelAccuracy %>% 
roc_curve(truth, .pred_1) %>%
  ggplot2:: ggplot(aes(x = 1 - specificity, y = sensitivity, fill = "LogisticModel", group = "1")) +
  ggplot2:: geom_path(aes( colour = unname(boe_cols)[1]), show.legend = F, lwd = 2) +
  ggplot2:: geom_abline(lty = 4, colour = unname(boe_cols)[3], show.legend = FALSE, lwd = 2) +
  ggplot2:: scale_colour_manual(values = rep(unname(boe_cols),1)) + 
  ggplot2:: coord_equal() +
  ggplot2:: theme_minimal(base_size = 16) + 
  ggplot2:: labs(subtitle = "Chart 3: Logistic regression model ROC")

```

## Machine Learning Classifiers

In this section an array of classifiers are explored. Each model is fitted to the data using a ‘training’ sample. The ‘testing’ sample is used to evaluate each model’s performance in predicting credit union failures. The model fits can be seen in the [Annex](#annex). Once the models have been fitted using the training data, each model's performance can be evaluated according to the criterion outlined in [Chapter 3](#modelfit), when applied to unseen data (out-of-sample). The results are shown in Table 3.2.

<br /> 

```{r fitmodel, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

#---------------------------------------------------------------------------------------
# fit random forst model using parsnip rand_forest function, fit using training data 
randomForest <- parsnip:: rand_forest(mtry = 10, trees = 150, min_n = 6, mode = "classification") %>% 
                parsnip:: set_engine("randomForest") %>%
                parsnip:: fit(DEFAULT ~ ., data = train_data)

# run boosted trees alogrithm and test performance 
boostedTree <- parsnip:: boost_tree(mtry = 10, trees = 100, min_n = 6, mode = "classification") %>% 
  parsnip:: set_engine("xgboost", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run decision tree and test model performance  
decisionTree <- parsnip:: decision_tree(mode = "classification", cost_complexity = 10, min_n = 6) %>% 
  parsnip:: set_engine("C5.0", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run KNN model 
KNNmodel <- parsnip:: nearest_neighbor(mode = "classification",neighbors = 10) %>% 
  parsnip:: set_engine("kknn", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run SVM models
SVMmodel <- parsnip:: svm_rbf(mode = "classification", rbf_sigma = 0.2) %>% 
  parsnip:: set_engine("kernlab", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run LDA models
LDAModel <- MASS:: lda(DEFAULT ~ ., data = train_data) 

# run naive bayes models
NaiveBayesModel <- e1071:: naiveBayes(DEFAULT ~ ., data = train_data) 

# run neural network model with tuning gride
tuning.grid <- expand.grid(.size=1:6, .decay=0, .bag=FALSE)  
NeuralNetModel.tuned <- caret:: train(DEFAULT ~ ., data = train_data, method="avNNet", trace=FALSE, tuneGrid=tuning.grid, linout=TRUE, repeats=1)


```


```{r modelresults, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

#---------------------------------------------------------------------------------------
# Output models 

load(file.path(basePath, "./Outputs/ModelResult.RData"))

ModelResult %>% 
  tidyr:: gather(key = model, value = value, -.metric) %>% 
  dplyr:: filter(.metric %in% c("accuracy", "sens", "spec", "bal_accuracy", "roc_auc")) %>% 
  tidyr:: spread(key = .metric , value = value) %>%
  dplyr:: mutate_if(is.numeric, ~formattable:: color_tile("lightpink", "lightgreen")(.)) %>%
  kableExtra:: kable(escape = F,table.attr = "style = \"color: black;\"",  caption = "Model performance statistics") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "left") #%>% 
  #kableExtra:: scroll_box(width = "100%")


```

<br /> 

Table 3.2 shows that the LDA, logistic, Naive Bayes and ANN models perform the best as evaluated using balanced accuracy. The Naive Bayes classifier outperforms the others in terms of specificity (56%), whilst the other three aforementioned models perform well. Chart 4 shows the ROC curve for the models listed above. 

<br /> 

```{r ROCplots, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

#---------------------------------------------------------------------------------------
# Output ROC results
load(file.path(basePath, "./Outputs/ModelAccuracy.RData"))

Logistic <- MakeROCPlot(data = ModelAccuracy, modelType = "logisticModel", title = "Logistic")
NaiveBayes <- MakeROCPlot(data = ModelAccuracy, modelType = "NaiveBayes", title = "Naive Bayes")
LDA <- MakeROCPlot(data = ModelAccuracy, modelType = "LDA", title = "LDA")
NeuralNet <- MakeROCPlot(data = ModelAccuracy, modelType = "NeuralNet", title = "Neural Network")

gridExtra:: grid.arrange(Logistic, NaiveBayes, LDA, NeuralNet, top = grid:: textGrob("Chart 4: ROC for machine learning classifiers",gp=gpar(fontsize=16,font=3)))

```

<br /> 

The steps to build a machine learning system include training, validation and testing. The next section of this chapter explores model validation, which is the process of calibrating the models to maximise out-of-sample performance. This may either mean the variation of a model’s degrees of freedom, like the number of nodes and layers in a neural network, tuning the hyper-parameters using a meta-algorithm or changing the rebalancing method.  

**NOTE:** The data training data has not been rebalanced (as explained in [Chapter 2](#modelsetup)) here, meaning only the standardisation pre-processing steps have been applied the data.

The code in the [Annex](#annex) shows the implementation of four rebalancing methods, using the LDA model. Here the number of folds is set to `k = 10`. The performance metrics are shown in Table 3.3. 

```{r recipe, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

#---------------------------------------------------------------------------------------
# prepare training and test data without upsampling 

simple_recipe <- function(dataset){
  recipes :: recipe(DEFAULT ~ ., data = dataset) %>%
    recipes:: step_center(all_numeric()) %>%
    recipes:: step_scale(all_numeric()) %>%
    recipes:: step_dummy(all_nominal(),- DEFAULT) 
}

train_recipe <- recipes:: prep(simple_recipe(train), training = train, retain = TRUE)
train_data <- train_recipe %>% juice 
test_data <- recipes:: bake(train_recipe, new_data = test) 


```


```{r rebalancePerformance, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

method = c("up", "down", "smote", "rose")

ModelPerformance <- lapply(X = c("up", "down", "smote", "rose"),  
                           FUN = trainControlSample, 
                           method = "nb", 
                           resamplingmethod = "boot",
                           traindata = train_data, 
                           testdata = test_data, 
                           numberFolds = 10) 

data <- ModelPerformance %>% purrr:: map(1) %>% purrr:: invoke(rbind, .)

rebalancePerformance <- tidyr:: spread(data, key = Meausure, value = Position)


```

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}
load(file.path(basePath, "./Outputs/rebalancePerformance.RData"))

rebalancePerformance %>% dplyr:: select(-resamplingmethod, -F1, -`Neg Pred Value`, -`Pos Pred Value`, 
                                        -`Detection Prevalence`, -`Detection Rate`, 
                                        -Precision, -Prevalence) %>% 
  dplyr:: mutate_if(is.numeric, ~formattable:: color_tile("lightpink", "lightgreen")(.)) %>%
  kableExtra:: kable(escape = F,table.attr = "style = \"color: black;\"",  caption = "Model performance statistics for different rebalancing methods") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "left") #%>% 
  #kableExtra:: scroll_box(width = "100%")


```

<br /> 

Using balanced accuracy, sensitivity and specificity as the performance measures, it appears as though down-sampling and up-sampling are comparable. Up-sampling is chosen as the best rebalancing method, which consists replicating some points from the minority class. The same result holds for the logistic, naive Bayes and neural networks (analysis omitted). The next step is to understand which of the four models performs best when subjected cross validation. Here the number of folds is set to `k = 5`. The cross-validation parameters are set as follows;

* The resampling method: `method = boot` (see annex)
* Number of folds: `number = 5`, 
* Rebalancing method: `sampling = up`

Each model is evaluated, using resampling, evaluating each instance to obtain an overall accuracy estimate. The implementation of cross-validation is shown in the [Annex](#annex) with the results displayed in Table 3.4. 

```{r modeloptimalPerformance, include=FALSE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

modeltypes =  c("lda", "plr", "nb", "nnet")

ModelPerformance <- lapply(X = modeltypes, 
                           FUN = trainControlSample, 
                           resamplingmethod = "boot",
                           samplingMethod = "up", 
                           traindata = train_data, testdata = test_data, 
                           numberFolds = 5)

data <- ModelPerformance %>% purrr:: map(1) %>% purrr:: invoke(rbind, .)

rebalancePerformance <- tidyr:: spread(data, key = Meausure, value = Position)


```

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}
load(file.path(basePath, "./Outputs/AllModels.RData"))

AllModels %>% dplyr:: select(-resamplingmethod,-samplingMethod, -F1, -`Neg Pred Value`, 
                             -`Pos Pred Value`, -`Detection Prevalence`, -`Detection Rate`, 
                             -Precision, -Prevalence) %>% 
  dplyr:: mutate_if(is.numeric, ~formattable:: color_tile("lightpink", "lightgreen")(.)) %>%
  kableExtra:: kable(escape = F,table.attr = "style = \"color: black;\"",  caption = "Model performance statistics, cross validated") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "left")# %>% 
  # kableExtra:: scroll_box(width = "100%")


```

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}
load(file.path(basePath, "./Outputs/ConfusionMatrices.RData"))
modeltypes = c("lda", "plr", "nb", "nnet")
cm = ConfusionMatrices

lda = data.frame(t(cm[[1]])) %>% tidyr:: spread(key = Prediction, value = Freq) %>%   
  dplyr:: mutate(`0` = formattable:: color_tile("lightpink", "lightgreen")(`0`), 
                 `1` = formattable:: color_tile("lightgreen", "lightpink")(`1`))

plr = data.frame(t(cm[[2]])) %>% tidyr:: spread(key = Prediction, value = Freq) %>%   
  dplyr:: mutate(`0` = formattable:: color_tile("lightpink", "lightgreen")(`0`), 
                 `1` = formattable:: color_tile("lightgreen", "lightpink")(`1`))

nb = data.frame(t(cm[[3]])) %>% tidyr:: spread(key = Prediction, value = Freq) %>%   
  dplyr:: mutate(`0` = formattable:: color_tile("lightpink", "lightgreen")(`0`), 
                 `1` = formattable:: color_tile("lightgreen", "lightpink")(`1`))

nnet = data.frame(t(cm[[4]])) %>% tidyr:: spread(key = Prediction, value = Freq) %>%   
  dplyr:: mutate(`0` = formattable:: color_tile("lightpink", "lightgreen")(`0`), 
                 `1` = formattable:: color_tile("lightgreen", "lightpink")(`1`))

kableExtra:: kable(lda, able.attr = "style = \"color: black;\"", escape = F, caption = "LDA Confusion matrix") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "float_left") %>%
  kableExtra:: add_header_above(c(" ", "Prediction" = 2)) %>% 
  kableExtra:: add_header_above(c("LDA" = 3))

kableExtra:: kable(plr, able.attr = "style = \"color: black;\"", escape = F, caption = "Logistic case") %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "center") %>%
  kableExtra:: add_header_above(c(" ", "Prediction" = 2)) %>% 
  kableExtra:: add_header_above(c("Logistic" = 3))

kableExtra:: kable(nb, able.attr = "style = \"color: black;\"", escape = F, caption = NULL) %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "float_left") %>%
  kableExtra:: add_header_above(c(" ", "Prediction" = 2)) %>% 
  kableExtra:: add_header_above(c("Naive Bayes" = 3))

kableExtra:: kable(nnet, able.attr = "style = \"color: black;\"", escape = F, caption = NULL) %>%
  kableExtra:: kable_styling(c("striped", "bordered"), full_width = FALSE, position = "center") %>%
  kableExtra:: add_header_above(c(" ", "Prediction" = 2)) %>% 
  kableExtra:: add_header_above(c("Neural Networks" = 3))


```

<br /> 

Looking at the performance measures, LDA outperforms the other models at 72% balanced accuracy and 62% specificity, which is comparable to the logistic model. The LDA model is successfully classifying `DEFAULT` cases around 62% of the time, and is raising few 'false flags' within the test set. The LDA correctly classifies 18 out of 29 `DEFAULT` observations; however the model raises 386 false positives and performs the worst as measured by sensitivity. The LDA sensitivity performance is much lower than the other classifiers, which is a counter to the supervisory preference of minimising the number of false flags of viable entities. The ANN performs best in terms of sensitivity (92%). The model correctly identifying 15 out of 29 `DEFAULT` observations, whilst raising significantly less false positives (171). The naive Bayes performs comparably with ANN. Given the overall specificity performance comparability between the LDA and neural networks, the neural network algorithm is considered preferable on the basis it produces significantly less false positives. Cross-validation is performed on the ANN by applying a set of permutations to the model’s degrees of freedom in an attempt to enhance predictive performance. This is explored in the next [Chapter 4](#cv). A simplified visual representation of the neural network (`hidden = 3`) can be seen below in Chart 5. 

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}
load(file.path(basePath, "./Outputs/NeuralNetwork.RData"))

# fit model with 3 layers, step max to ensure convergence
#set.seed(2)
#NN = neuralnet(DEFAULT~., train_data, hidden = 3 , linear.output = F, stepmax = 1e6)

# plot neural network
NeuralNetTools:: plotnet(NN, bord_col = "#1E1E1E", circle_col = "#A51140", circle_cex = 3, alpha_val = 0.5, pos_col = "#005E6E",  neg_col = "#CAC0B6") 

graphics:: title(main = list("Chart 5: Neural interpretation diagram with 3 hidden layers", cex = 1,
                   col = "black", font = 1))


```

# Cross Validation {#cv}

## Model Tuning

One way to improve a model is cross-validation (cv), which applies the algorithms over different sets of parameters or degrees of freedom, then comparing the model performance over these sets. K-fold cv is applied to the neural network, whilst varying the degrees of freedom in the model. The model is computed over a tuning grid consisting of the `size` (units in hidden layer) and `decay` (regularization parameter to avoid over-fitting). The set within the tuning grid is selected based on the best out of sample performance, which can be seen below;

```{r , include=FALSE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

fitControl <- trainControl(
  method = "repeatedcv", # resampling method
  number = 5, # k = 5 folds
  repeats = 5) # repeats 

tune.grid.neuralnet <-  expand.grid(size = seq(from = 1, to = 20, by = 2),
                                    decay = seq(from = 0.1, to = 1, by = 0.1))


set.seed(825)
nnetFit <- caret:: train(DEFAULT ~ ., data = train_data, 
                         method = "nnet", 
                         trControl = fitControl, 
                         verbose = FALSE, 
                         ## Now specify the different degrees of freedom to apply 
                         tuneGrid = tune.grid.neuralnet)


```

Chart 4 shows the results of the cv exercise. As measured by overall accuracy, the model performance is maximised for `size = 19` and `decay = 0.1`. 

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

load(file.path(basePath, "./Outputs/nnetFit.RData"))

p1 <- ggplot2::ggplot(nnetFit, linemetre)  +
  ggplot2:: scale_colour_manual(values = rep(unname(boe_cols),1)) + 
  ggplot2:: theme_minimal(base_size = 16) + 
  ggplot2:: labs(y = paste0("Accuracy")) 

p2 <- ggplot2:: ggplot(nnetFit, plotType = "level", scales = list(x = list(rot = 90))) + 
  ggplot2:: theme_minimal(base_size = 16) + 
  ggplot2:: scale_fill_gradient(low = "white", high = "darkred") 

gridExtra:: grid.arrange(p1, p2, top = grid:: textGrob("Chart 4: Cross validation of neural network model over tuning grid",gp=gpar(fontsize=16,font=3))) #+ ggplot2:: labs(subtitle = paste0("Cross validation of neural network model over tuning grid")) 

```

## Bias-variance trade off 

When it comes to machine learning, there is a trade of between model performance and cost. Basic models with few degrees of freedom (logistic case) are often simple to calculate (low cost); however may lead to poorer model fits and performance (e.g. under-fitting, when there is a non-linear relationship). On the other hand, sophisticated models such as ANN can provide more accurate fits, as demonstrated above but are computationally intensive (cv ANN > 10mins to compute). In addition, complex models with a large number of parameters can lead to overfitting or be subject to a lot of variance (bias-variance trade off) [@brett2019]. The cv process, as demonstrated in [Section 2](#cv) can help calibrate a model’s fit, which in turn can improve predictive performance.

## Overfitting 

ANN have a lot of freedom; however a lack of control over the learning process can lead to overfitting. This is a situation when the neural network is so closely fitted to the training set that it is difficult make accurate out of sample predictions on previously unseen data. When a given method yields a small training error (low cost) but a large test error, then this is a sign of overfitting the data [@daniel2015]. One method to reduce overfitting is regularisation, which introduces a loss function that penalises the model for being too complex [@brownlee2015].


## Scalability 

Thus far, only the accuracy of the model has been considered. The scalability of the machine learning algorithm is of importance when implementing such solutions. ANN are computationally intensive and slow to train. The credit union data consists of `~112,000` observations, which is not considered a large-scale database; however even with this modest dataset, cv and feature analysis was time consuming (using parallelisation `doParallel`). It is possible run these algorithms on a local machine; however given the ever growing set of records, one day this may become a problem. Feature selection can be helpful to reduce dimensionality, which selects only the most relevant features, [Chapter 5](#feature). The library 'sparklyr' is a potential solution to this, which is a distributed processing system used for big data workloads. The implementation of this would look something like the following;


```{r , include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

sc <- spark_connect(master = "local")
DataML <- sdf_copy_to(sc, DataML, name = "mtcars_tbl", overwrite = TRUE)

partitions <- DataML %>%
  sdf_random_split(training = 0.7, test = 0.3, seed = 1111)

training <- partitions$training
test <- partitions$test

logistic.model <- training %>%
  ml_logistic_regression(DEFAULT ~  .)

pred <- ml_predict(logistic.model, test)

ml_binary_classification_evaluator(pred)


```

# Feature importance {#feature}

This section explores the overall feature importance in the model. Neural networks and other machine learning models are often referred to as 'black box' models. This is because these models lack explicability and interpretability, since the layers and operations applied in the algorithms are not visible or easy to understand. This is sometimes hard look past, especially in the case of firm supervision.

The relative importance of individual features can give end-users a better understanding of a model’s output. It is important to understand the source of the classifications, namely the default classes `DEFAULT = 1`, in the context of the data. One approach to understanding the contribution of a feature in a model is to observe the amount by which the inclusion or exclusion of a feature changes the performance.

Chart 5 shows the relative importance of the features, based on the ANN fitted in [Section 3](#cv). 

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = T}

importance <- caret:: varImp(nnetFit, scale=FALSE)
# summarize importance
print(importance)

```

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

data <- tibble:: tibble(Importance = round(importance$importance$Overall, 1), Feature = row.names(importance$importance))
colnames(data) <- c("Importance", "Feature")

ggplot2:: ggplot(data, aes(x=reorder(Feature, Importance), y= Importance, label=Importance)) + 
  ggplot2:: geom_point(stat='identity', fill= "#A51140", size=12)  +
  ggplot2:: geom_segment(aes(y = 0, 
                   x = Feature, 
                   yend = Importance, 
                   xend = Feature), 
               color = "#A51140") +
  ggplot2:: geom_text(color="white", size=3) + 
  ggplot2:: labs(title="", 
       subtitle="Chart 5: Relative feature importance", y = "Importance", x = NULL) +
  ggplot2:: coord_flip() + 
  ggplot2:: theme_minimal(base_size = 16) +     
  ggplot2:: theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank(), 
    panel.grid.major = element_blank()) 

```

Chart 5 shows that unsecured assets, unemployment, loan to deposit ratio and log assets are the most important features in the model. This illustrates the pronounced effects of unemployment and the importance considering macroeconomic as well as microeconomic factors. Note that the number of features doesn’t have to be the same as the explanatory predictors. Adding an interactional or functional set of features can capture other complexities in the data. By doing so it may be possible to enhance predictive performance by enriching the dataset. In some cases the data is not separable when presented in its raw form; however when re-engineered, features can help distinguish between the classes and, so, improve the classifier accuracy.

Recursive Feature Elimination (RFE) is a feature selection method that fits a model and removes the weakest feature. The method uses Random Forest algorithm to evaluate the model, which is configured to explore all possible subsets of the attributes [@brownlee2014]. The results of the RFE are shown below in Chart 6.


```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = FALSE}

# define the control using a random forest selection function
control <- caret:: rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- caret:: rfe(DEFAULT~.,data = train_data, sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)

```

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

load(file.path(basePath, "./Outputs/RDEresults.RData"))
# plot the results
ggplot(results) + 
  #ggplot2:: geom_line(colour= "#A51140", size=1.2) + 

  ggplot2:: theme_minimal() + 
  ggplot2:: labs(title="", 
                 subtitle="Chart 6: Recursive Feature Elimination Selection", y = "Accuracy", x = "Number of Variables") +
  #ggplot2:: coord_flip() + 
  ggplot2:: theme_minimal(base_size = 16) +     
  ggplot2:: theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank(), 
    panel.grid.major = element_blank()) +
  ggplot2:: scale_y_continuous(breaks = scales:: pretty_breaks(n = 10))  +
  ggplot2:: scale_x_continuous(breaks = scales:: pretty_breaks(n = 10))

```

Chart 6 implies that the optimal number of features is six. The most ‘important’ features in the data under RFE are displayed below. Note this is broadly in agreement with results displayed in Chart 5.

```{r , include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

print(results$optVariables)

```


# Conclusion {#conclusion}

This paper introduced an array of supervised machine learning classification algorithms and explored the effectiveness of each in predicting credit union defaults. The classifiers included artificial neural networks, support vector machines, naive Bayes, linear discriminant analysis, support vector machines and tree algorithms, which were all measured against the baseline fit in the form of a logistic model. The baseline logistic, naive Bayes, LDA and neural networks performed best as measured by balanced accuracy and specificity, which helped capture the supervisory preference between Type I error and a Type II error. The four aforementioned classifiers were taken forward to undergo model calibration, which consisted of changing the rebalancing method, changing the resampling method and varying the model’s degrees of freedom. It resulted that there ANN performed best, on the basis its Type I error was comparable to the other models, whilst its Type II error outperformed the other classifiers significantly. The neural network itself poses some challenges, namely overfitting and the interpretability of the results (‘black box methods’).

This analysis highlighted that there is no ‘fixed’ machine learning model, but a large array of model types, each of which consist of their own degrees of freedom, sets of hyper-parameters and different interpretations of ‘success’ as captured by the performance metrics. The ‘success’ is a combination of the right model ingredients, but also depends on the knowledge and understanding of the end user (supervisory teams), whilst keeping one eye on the end goal, that is to flag firms that are at risk of default, whilst minimising the overall number of false flags of perfectly viable entities. It follows that the ANN model is the best when considering these trade-offs; however it could be limited in its application due to its lack of interpretability and scalability due to computational challenges. In light of this, other classification algorithms may be preferred; however, in this paper, artificial neural networks demonstrated its ability to uncover nuanced relations between variables and account for heterogeneity amongst enitities.

# References

<div id="refs"></div>

 <br /> 
 

# Appendix {#annex}

## Extensions 

### Multi-step forecast

This analysis was conducted using a static approach, i.e. a model that is looking to predict credit union defaults during in a single reporting period. Based on a regulatory returns submitted to the Bank of England, firm information could be used to help classifying entities into one of two categories, `DEFAULT` and `NON DEFAULT`. The firms falling into the first category could then be triaged and put forward for further investigation. This is known as a One-Step Forecast, which is where the next time step (t+1) is predicted.

It is possible to extend the machine learning approach to model failure using a multi-period model since there is an appetite from supervisors to anticipate failure regardless time period. This could be incorporated into the model by including time dummy variables (year effects) in the feature set. This is known as a Multi-Step Forecast, which is where two or more future time steps (t+1, t+2, t+3, ..., t+n) are to be predicted. It would be expected that over a given time horizon over which we predict failure, model performance would decline; however this could provide supervisors with early warning indicators before it is too late, thus allowing for pre-emptive action to be taken.

Under the multip step approach, the additional set of features would look something like follows;

```{r extensions, include=TRUE, warning=FALSE, message=FALSE, echo=FALSE, eval = TRUE}

library(data.table)
library(stargazer)
basePath = file.path("//researchhub/files/May/Credit Union Failure/SRDDfolder")

# source data from files
dataRaw = data.table:: fread(file.path(basePath, "appdatawin.csv"), stringsAsFactors = FALSE)

#select columns to keep
cols = colnames(dataRaw)[which(!grepl(pattern = paste0(c("FRN", "firm", "year"), collapse = "|"), x = colnames(dataRaw), ignore.case = TRUE))]
#prepare data 
dataClean = dataRaw[, match(cols, colnames(dataRaw)), with = FALSE]

y <- dataClean[, c("DEFAULT","failurel1","failurel2", "failurel3","failurel4"), with = FALSE]
head(y)


```

### Model tuning 

The models explored in this exercise peform relatively in predicting credit union defaults when accounting for supervisory preferences between Type I and Type II erros. Model performance could be further enhanced via the application of some of the following techniques;

1. **Regularisation**: This is a smoothing method that introduces a penalty term to the error function acting on non-zero, especially large, parameter values. Its effect is the reduction of the complexity, or the smoothening, of a model. This is useful when faced with an over-fitting model but also for feature selection, which can sometimes be problematic in neural networks. 

2. **Bagging**: A popular method for improving an estimator’s properties. The idea is that, when aggregating (averaging) the outputs of many estimators, one reduces the variance of the full model. When over-fitting is an issue, as for tree
models, bagging is likely to improve performance. 

3. **Boosting**: Describes a set of methods which transform, or boost, a weak learner into a strong one (model). This means that one iteratively applies models to the data which individually have weak generalisation properties, but the final ensemble of models generalises well.

4. **Simulated data**, which involves creating synthetic data of the minority class (`DEFAULT = 1`) to help rebalance the minority class. This could be achieved with the help of supervisor’s specialist knowledge of the entities themselves and the associated risks. The result could be better out of sample predictive performance as the model has 'seen' more instances of what a `DEFAULT` looks like and is therefore more likely to be able to identify future instances.


## Source Code

All code can be found on the git repository here [CreditUnionFailureML](https://almplatform/tfs/UnmanagedCollection/ProjRDG/_git/CreditUnionFailureML). The code snippits below outline the steps taken in conducting this analysis, from data preperation, model fitting, cross-validation, performance evaluation and testing. 

Below is the list of helper functions created to support the analysis. 

```{r Functions, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

# helper funs

#' @title BOE colour pallete used for charting  

boe_cols <- c(
  red            = "#A51140",
  black          = "#1E1E1E",
  stone          = "#CAC0B6",
  dark_teal      = "#005E6E",
  dark_blue      = "#002A42",
  plum           = "#752864",
  mid_blue       = "#165788",
  maroon         = "#6C0721",
  purple         = "#4E3780",
  grey           = "#999999",
  green          = "#006663", 
  orange         = "#D55E00", 
  orange2        = "#E69F00", 
  blue           = "#56B4E9")

MakeROCPlot <- function(data, modelType, title){
  
  ModelAccuracy %>% 
    dplyr:: filter(imbalanceMethod ==  paste0(modelType)) %>% 
    yardstick:: roc_curve(truth, .pred_1) %>%
    ggplot2:: ggplot(aes(x = 1 - specificity, y = sensitivity, fill = paste0(modelType), group = "1")) +
    ggplot2:: geom_path(aes( colour = unname(boe_cols)[1]), show.legend = F, lwd =2) +
    ggplot2:: geom_abline(lty = 4, colour = unname(boe_cols)[3], show.legend = FALSE, lwd = 2) +
    ggplot2:: scale_colour_manual(values = rep(unname(boe_cols),1)) + 
    ggplot2:: coord_equal() +
    ggplot2:: theme_minimal(base_size = 16) + 
    ggplot2:: labs(subtitle = paste0(title)) 
  
  
}


PrepForRecipe <- function(df, ...){
  
  for(i in 2:nrow(df)) {
    
    if((nrow(df) %% i) == 0) {
      
      i
      
      break()
      
    }
    
  }

  df <- df[,which(!colnames(df) %in% c("FRN","FirmName", "Year"))]
  
  df$DEFAULT <- as.factor(df$DEFAULT)
  
  return(df)
  
}

simple_recipe <- function(df){
  
  recipe(Raise ~ ., data = df) %>%
    step_center(all_numeric()) %>%
    step_scale(all_numeric()) %>%
    step_dummy(all_nominal(),-Raise) %>% 
    step_upsample(Raise, ratio = 0.5)
  
  
}

yncount <- function(df){
  
  y <- countraise(df, "Yes") 
  
  print(y)
  
  n <- countraise(df, "No") 
  
  print(n)
  
} 

##Function to obtain the perdicted class from the model and test data along with

#Probabilty scores.
predictAndProb <- function(model, testData,imbalanceMethod){
  
  pred_raw = predict(model, new_data = testData,  type = "raw")
  pred_probs = predict(model, new_data = testData,type = "prob") 
  pred_class = predict(model, new_data = testData, type = "class")
  
  #Building the predictions of the model and the data with probablity
  predictions = tibble:: tibble(truth = testData$DEFAULT, predicted = pred_class$.pred_class) %>% 
    cbind(pred_probs) %>% 
    dplyr:: mutate(imbalanceMethod = imbalanceMethod)
  
  return(as_tibble(predictions))
}

##Code that extracts performance metrics after running the model, this is a single
#model not one that is cross validated.

modelResults <- function(model, testData, type){
  
  modelAccuracy <- predictAndProb(model, testData, type)
  
  confMatrix <- yardstick:: conf_mat(modelAccuracy, truth, predicted)
  confMatrixStats <- summary(confMatrix)
  auc <- yardstick:: roc_auc(modelAccuracy, truth, .pred_1)

  modelStats <- rbind(confMatrixStats, auc) %>% dplyr:: mutate(type = type)
  
  plot <- modelAccuracy %>% 
    yardstick:: roc_curve(truth, .pred_1) %>% 
    autoplot
  
  return(
  list(confMatrix = confMatrix, 
       modelStats = modelStats,
       modelAccuracy = modelAccuracy,
       plot = plot)
  )
}


#------------------------------------------------------------------------------


trainControlSample <- function(traindata, testdata, samplingMethod, method, resamplingmethod, numberFolds = 10, repeats = 5){
  
  ctrl <- trainControl(method = paste0(resamplingmethod), number = numberFolds, verboseIter = FALSE,  repeats = repeats, sampling = paste0(samplingMethod))
  
  set.seed(42)
  model_trained <- caret::train(DEFAULT ~ .,
                                data = traindata,
                                method = paste0(method),
                                #preProcess = c("scale", "center"),
                                trControl = ctrl)
  
  finalPreds <- data.frame(actual = testdata$DEFAULT, predict(model_trained, newdata = testdata, type = "prob"))
  
  finalPreds$predict <- as.factor(ifelse(finalPreds$X1 > 0.5, 1, 0))
  cm_over <- confusionMatrix(finalPreds$predict, testdata$DEFAULT) 
  
  Results <- data.table:: data.table(cm_over$byClass)
  colnames(Results)[1] = "Position"
  Results <- Results[, `:=`(Meausure = names(cm_over$byClass), 
                            resamplingmethod = paste0(resamplingmethod), 
                            samplingMethod = paste0(samplingMethod), 
                            modeltype = paste0(method))]
  
  return(list(Results, cm_over$table))
  
  
}

```

### Chapter 2 

The code below shows steps taken in preparing the data for the analysis as discussed in [Chapter 2](#modelsetup). This includes reading the data, transforming the data to be structured for a machine learning problem and outputting the summary statistics. 

```{r dataprepcode1, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

library(tidyverse)
library(data.table)
library(stargazer)

basePath = file.path("//researchhub/files/May/Credit Union Failure/SRDDfolder")

# source data from files
dataRaw = data.table:: fread(file.path(basePath, "appdatawin.csv"), stringsAsFactors = FALSE)

#select columns to keep
cols = colnames(dataRaw)[which(!grepl(pattern = "failure", x = colnames(dataRaw), ignore.case = TRUE))]
#prepare data 
dataClean = dataRaw[, match(cols, colnames(dataRaw)), with = FALSE][, FirmName := Firm.Name.x][,-"Firm.Name.x"]

variables<-c("roa", "pctarrears", "loandeposit", "simplecapitalratio",
             "provlns", "logassets", "unsecassets", "CY_30E", "arrears312", "arrearsg12",
             "CPI", "GDP.Growth.Quarter.Last.Year", "Base.Rate", "Regunemployment")

y <- dataClean[, variables, with = FALSE]

colnames(y)<- c("Return on Assets (%)","Arrears rate (%)", "Loan-deposit ratio (%)", "Risk-adjusted capital ratio (%)",  "Provisions-loans ratio (%)", paste0("Log assets(", "\u00A3", ")"), "Unsecured loans to assets (%)", "Capital ratio (%)", "Arrears < 12 Months (%)", "Arrears > 12 Months (%)", "Inflation (%, CPI)", "GDP Growth (Quarterly, %)",  "Bank Rate (%)", "Regional Unemployment Rate (%)")

stargazer:: stargazer(y, title="Summary Statistics", type="html", out = "./Outputs/summarystats.html", digits=1)

#--------------------------------------------------------------------------------------------------------------------------


```

The code below shows steps taken in preparing the data for machine learning classifiers as discussed in [Chapter 2](#modelsetup). This includes creating the training and test splits. 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}

variables<-c("FRN", "FirmName", "Year", "DEFAULT", "roa", "pctarrears", "loandeposit", "simplecapitalratio",
             "provlns", "logassets", "unsecassets", "CapitalRatio", "arrears312", "arrearsg12",
             "CPI", "GDP.Growth.Quarter.Last.Year", "Base.Rate", "Regunemployment")

DataML <- dataClean[, variables, with = FALSE]
DataML <- na.omit(DataML)

# prepare data for ML analysis
DataML <- PrepForRecipe(as.data.frame(DataML))
nrow_DataML <- tibble:: tibble(`Before Split` = nrow(DataML))

set.seed(12345)
train_test_split <- rsample:: initial_split(DataML, prop = 0.7)
train <- rsample:: training(train_test_split) # create training set 
test <- rsample:: testing(train_test_split) # create test set 

nrow_train_data <- tibble:: tibble(`Training set` = nrow(train))
nrow_test_data <- tibble:: tibble(`Test set` = nrow(test))

table <- cbind(nrow_DataML, nrow_train_data, nrow_test_data)
kableExtra:: kable(table, caption = "Training and test sets") %>% 
  kableExtra:: kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F, position = "left")

```

The final step before the models are fitted is the prep-processing and standardisation of the data, which is shown below. 

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}

#---------------------------------------------------------------------------------------
# Create 'recipe' for pre-processing input data, and apply to one example split of data --------

simple_recipe <- function(dataset){
  
  recipes:: recipe(DEFAULT ~ ., data = dataset) %>%
    recipes:: step_center(all_numeric()) %>%
    recipes:: step_scale(all_numeric()) %>%
    recipes:: step_dummy(all_nominal(),- DEFAULT) %>%  
    recipes:: step_upsample(DEFAULT, ratio = 0.5) # this step rebalances the data 
  
}

train_recipe <- recipes:: prep(simple_recipe(train), training = train, retain = TRUE) # prepare data model, apply processing steps
train_data <- train_recipe %>% juice  # generate training set 
test_data <- recipes:: bake(train_recipe, new_data = test) # generate test set 

table <- rbind(c(CountDefaultsTrain = sum(train_data$DEFAULT == 1), CountNonDefaultsTrain = sum(train_data$DEFAULT == 0)), 
               c(CountDefaultsTest = sum(test_data$DEFAULT == 1), CountNonDefaultsTest = sum(test_data$DEFAULT == 0)))

rownames(table) <- c("training data", "test data")


```

### Chapter 3 

The first step is to fit the models, using a set of classifiers identified in this paper as discussed in [Chapter 3.3](#modelfit). The first is the logistic case, which is fitted as follows;

```{r, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

#---------------------------------------------------------------------------------------
# fit logistic model using parsnip logistic_reg function, fit using training data 
logisticModel <- parsnip:: logistic_reg(mode = "classification", penalty = "glm") %>% 
  parsnip:: set_engine("glm") %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

#obtain model results using function (see annex for details) applying to the test data 
logisticModelResults <- modelResults(logisticModel, testData = test_data, "logisticModel")


```

The next step is to fit all the classifiers to the data. The code below shows the implemtation of fitting eight models to the credit union data. Note LDA, ANN and Naive Bayes are not included in the `parnsip` packages, so packages purposely build for applying these alogrithms are used, which also changes the way we collect the performance stats in the next section. 

```{r fitmodel1, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

#---------------------------------------------------------------------------------------
# fit random forst model using parsnip rand_forest function, fit using training data 
randomForest <- parsnip:: rand_forest(mtry = 10, trees = 150, min_n = 6, mode = "classification") %>% 
                parsnip:: set_engine("randomForest") %>%
                parsnip:: fit(DEFAULT ~ ., data = train_data)

# run boosted trees alogrithm and test performance 
boostedTree <- parsnip:: boost_tree(mtry = 10, trees = 100, min_n = 6, mode = "classification") %>% 
  parsnip:: set_engine("xgboost", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run decision tree and test model performance  
decisionTree <- parsnip:: decision_tree(mode = "classification", cost_complexity = 10, min_n = 6) %>% 
  parsnip:: set_engine("C5.0", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run KNN model 
KNNmodel <- parsnip:: nearest_neighbor(mode = "classification",neighbors = 10) %>% 
  parsnip:: set_engine("kknn", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run SVM models
SVMmodel <- parsnip:: svm_rbf(mode = "classification", rbf_sigma = 0.2) %>% 
  parsnip:: set_engine("kernlab", importance = 'impurity') %>%
  parsnip:: fit(DEFAULT ~ ., data = train_data)

# run LDA models
LDAModel <- MASS:: lda(DEFAULT ~ ., data = train_data) 

# run naive bayes models
NaiveBayesModel <- e1071:: naiveBayes(DEFAULT ~ ., data = train_data) 

# run neural network model with tuning gride
tuning.grid <- expand.grid(.size=1:6, .decay=0, .bag=FALSE)  
NeuralNetModel.tuned <- caret:: train(DEFAULT ~ ., data = train_data, method="avNNet", trace=FALSE, tuneGrid=tuning.grid, linout=TRUE, repeats=1)


```


The code below shows the model performance analysis used in [Chapter 3.3](#modelfit). 

```{r modelresultscode1, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

#---------------------------------------------------------------------------------------
# Output models results using parnsip ouputs (RF, BT, DT, KNN, SVM)
randomForestResults <- modelResults(randomForest, testData = test_data, "RandomForest")
boostedTreeResults <- modelResults(boostedTree, testData = test_data, "boostedTree")
decisionTreeResults <- modelResults(decisionTree, testData = test_data, "decisionTree")
KNNTreeResults <- modelResults(KNNmodel, testData = test_data, "KNN")
SVMResults <- modelResults(SVMmodel, testData = test_data, "SVM")

#---------------------------------------------------------------------------------------
# Output models results using LDA
pred_raw = predict(LDAModel,test_data, type = "raw")
pred_probs = predict(LDAModel, test_data, type = "prob") 
pred_class = predict(LDAModel, test_data,type = "class")

###Building the predictions of the model and the data with probablity
predictions = tibble:: tibble(truth = test_data$DEFAULT, predicted = pred_class$class) 
LDAconfMatrix <- yardstick:: conf_mat(predictions, truth, predicted)
LDAconfMatrixStats <- summary(LDAconfMatrix)
auc <- yardstick:: roc_auc(ModelAccuracy , as.factor(truth), class)
LDAmodelStats <- rbind(LDAconfMatrixStats, auc) %>% dplyr:: mutate(type = "LDA")

#---------------------------------------------------------------------------------------
# Output models results using naive bayes
pred_raw = predict(NaiveBayesModel,test_data, type = "raw")
pred_class = predict(NaiveBayesModel, test_data,type = "class")

##Building the predictions of the model and the data with probablity
predictions = tibble:: tibble(truth = test_data$DEFAULT, predicted = pred_class)

NaiveBayesconfMatrix <- yardstick:: conf_mat(predictions, truth, predicted)
NaiveBayeconfMatrixStats <- summary(NaiveBayesconfMatrix)
NaiveBayeauc <- yardstick:: roc_auc(ModelAccuracy , as.factor(truth), class)
NaiveBayemodelStats <- rbind(NaiveBayeconfMatrixStats, NaiveBayeauc) %>% dplyr:: mutate(type = "NaiveBayes")

#---------------------------------------------------------------------------------------
# Output models results using neurol networks 

pred_raw = predict(NeuralNetModel.tuned, test_data, type = "raw")
pred_class = predict(NeuralNetModel.tuned, test_data,type = "prob")

class <- as.data.frame(pred_class) %>% dplyr:: mutate(class = if_else(`0` > `1`, 0, 1), class = as.factor(class)) %>% dplyr:: select(class)

colnames(class)[1] <- "class"
##Building the predictions of the model and the data with probablity
predictions = tibble:: tibble(truth = test_data$DEFAULT, predicted = class$class) 

ModelAccuracy <- tibble:: as_tibble(predictions) %>% dplyr:: mutate_if(is.factor, as.numeric)
NeuralNetModelconfMatrix <- yardstick:: conf_mat(predictions, truth, predicted)
NeuralNetModelconfMatrixStats <- summary(NeuralNetModelconfMatrix)
NeuralNetModelauc <- yardstick:: roc_auc(ModelAccuracy , as.factor(truth), predicted)
NeuralNetModelmodelStats <- rbind(NeuralNetModelconfMatrixStats, auc) %>% dplyr:: mutate(type = "NeuralNetwork")


```

Once the performance has been evaluated, four classifiers are taken forward for further analysis. The first step is the re-apply the preprocessing step of up-sampling, as this is implemented in the cross-validation process. This is shows in the code chunk below;


```{r, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval=FALSE}

#---------------------------------------------------------------------------------------
# prepare training and test data without upsampling 

simple_recipe <- function(dataset){
  recipes :: recipe(DEFAULT ~ ., data = dataset) %>%
    recipes:: step_center(all_numeric()) %>%
    recipes:: step_scale(all_numeric()) %>%
    recipes:: step_dummy(all_nominal(),- DEFAULT) 
}

train_recipe <- recipes:: prep(simple_recipe(train), training = train, retain = TRUE)
train_data <- train_recipe %>% juice 
test_data <- recipes:: bake(train_recipe, new_data = test) 


```

The next step is to apply different rebalncing methods to understand which performs the best in terms of predictive performace on the test data. 

```{r rebalancePerformance1, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

method = c("up", "down", "smote", "rose")

ModelPerformance <- lapply(X = c("up", "down", "smote", "rose"),  
                           FUN = trainControlSample, 
                           method = "nb", 
                           resamplingmethod = "boot",
                           traindata = train_data, 
                           testdata = test_data, 
                           numberFolds = 10) 

data <- ModelPerformance %>% purrr:: map(1) %>% purrr:: invoke(rbind, .)

rebalancePerformance <- tidyr:: spread(data, key = Meausure, value = Position)


```

Once the upsampling method is chosen as the best rebalancing method, the best classifiers are put forward for cross validation as shown below. 

```{r rebalancePerformance2, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

modeltypes = c("lda", "plr", "nb", "nnet")

ModelPerformance <- lapply(X = modeltypes, FUN = trainControlSample, resamplingmethod = "boot", samplingMethod = "up", 
    traindata = train_data, testdata = test_data, numberFolds = 5)

data <- ModelPerformance %>% purrr::map(1) %>% purrr::invoke(rbind, .)

rebalancePerformance <- tidyr::spread(data, key = Meausure, value = Position)


```

### Chapter 4 

The code below shows the model performance analysis used in [Chapter 4](#cv). 

```{r cvresultscode, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

#---------------------------------------------------------------------------------------------------
# model performance over resampling methods (k fold - e..g bootstrapping, repeated cv)
resamplingmethods <- c("boot", "boot632", "optimism_boot", "boot_all", "cv", "repeatedcv", "LGOCV")

ModelPerformance <- lapply(X = resamplingmethods, 
                           FUN = trainControlSample,  
                           data = test_data, 
                           numberFolds = 10, 
                           samplingMethod = "up", 
                           method = "lda") %>%  
  data.table:: rbindlist()

ModelPerformance <- tidyr:: spread(ModelPerformance, key = Meausure, value = Position)

#-------------------------------------------------------------------------------
# model performance over rebalancing methods 
rebalancingMethods <- c("up", "down", "smote", "rose")

ModelPerformance <- lapply(X = rebalancingMethods,  
                           FUN = trainControlSample, 
                           method = "lda", # can apply for others including nb and nnet
                           resamplingmethod = "boot", #best performing 
                           traindata = train_data, 
                           testdata = test_data, 
                           numberFolds = 10) 

data = ModelPerformance %>% purrr:: map(1) %>% purrr:: invoke(rbind, .)

rebalancePerformance <- tidyr:: spread(data, key = Meausure, value = Position)

#-------------------------------------------------------------------------------
# model performance over model type
modeltypes <- c("lda", "glm", "nb", "nnet")
ModelPerformance <- lapply(X = modeltypes, 
                           FUN = trainControlSample, 
                           resamplingmethod = "boot", #best performing 
                           samplingMethod = "up", #best performing 
                           traindata = train_data, 
                           testdata = test_data,
                           numberFolds = 5) 
  

AllModels <- ModelPerformance %>% purrr:: map(1) %>% purrr:: invoke(rbind, .) %>% tidyr:: spread(key = Meausure, value = Position)

ConfusionMatrices <- ModelPerformance %>% purrr:: map(2) #%>% purrr:: invoke(rbind, .)



```

The code below shows steps taken to cross-validate and tune the neural network model 

```{r neuralcvcode, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}
library(doParallel) #parallelize code for rfe algorithm

cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)

#--------------------------------------------------------------------------------------

fitControl <- trainControl(
  method = "repeatedcv", # resampling method
  number = 5, # k = 5 folds
  repeats = 5, 
  sampling = "up") # repeats 

tune.grid.neuralnet <-  expand.grid(size = seq(from = 1, to = 20, by = 2),
                                    decay = seq(from = 0.1, to = 1, by = 0.1))

nrow(tune.grid.neuralnet)

set.seed(825)

nnetFit <- caret:: train(DEFAULT ~ ., data = train_data, 
                         method = "nnet", 
                         trControl = fitControl, 
                         verbose = FALSE, 
                         ## Now specify the different degrees of freedom to apply 
                         tuneGrid = tune.grid.neuralnet)


```

### Chapter 5 

The code below shows the steps taken to carry out feature importance analysis in [Chapter 5](#cv). 

```{r featureimpcode, include=TRUE, warning=FALSE, message=FALSE, echo=TRUE, eval = FALSE}

load(file.path(basePath, "./Outputs/nnetFit.RData"))
#compute variable importance 
importance <- caret:: varImp(nnetFit, scale=FALSE)
# summarize importance
print(importance)

data <- tibble:: tibble(Importance = round(importance$importance$Overall, 1), Feature = row.names(importance$importance))
colnames(data) <- c("Importance", "Feature")

control <- caret:: trainControl(method="repeatedcv", number=10, repeats=3)
model <- caret:: train(DEFAULT ~ ., data= train_data, method="nnet", trControl=control)

imp<-varImp(model)
plot(imp)

#----------------------------------------------------------------------------------
# define the control using a random forest selection function
library(doParallel) #parallelize code for rfe algorithm

cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)

control <- caret:: rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- caret:: rfe(DEFAULT~.,data = train_data, sizes=c(1:8), rfeControl=control)
save(results, file =  "./Outputs/RDEresults.RData")

# plot the results
ggplot2:: ggplot(results, type=c("g", "o")) + 
  ggplot2:: geom_line(colour= "#A51140", size=1)  +
  ggplot2:: theme_minimal() + 
  ggplot2:: labs(title="", 
                 subtitle="Chart 5: Relative feature importance", y = "Importance", x = NULL) +
  ggplot2:: coord_flip() + 
  ggplot2:: theme_minimal(base_size = 16) +     
  ggplot2:: theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank(), 
    panel.grid.major = element_blank()) 


```
