[
["report.html", "Predicting Credit Union Failure - A Machine Learning Approach 1 Abstract 2 Introduction 3 Data Preparation 4 Model Fitting 4.1 Measuring Performance 4.2 Baseline Logistic Model 4.3 Machine Learning Classifiers 5 Cross Validation 5.1 Model Tuning 5.2 Bias-variance trade off 5.3 Overfitting 5.4 Scalability 6 Feature importance 7 Conclusion 8 References 9 Appendix 9.1 Extensions 9.2 Source Code", " Predicting Credit Union Failure - A Machine Learning Approach Joshua Allen, PRA Data Innovation 2020-05-26 The views expressed in this paper are those of the authors, and not necessarily those of the Bank of England or any of its committees. This paper was undertook as part of a univeristy assignment and is an extension of past and present ongoing research in the bank. For more information on the original research please see the Staff Working Paper No. 658 The determinants of credit union failure in the United Kingdom: how important are macroeconomic factors? All the analysis scripts are stored and version controlled in TFS under CreditUnionFailureML. Code snippets are included throughout this paper for the benefit of the readers either looking to reproduce this analysis or apply similar techniques elsewhere. Contacts: Bank of England. Email: joshua.allen@bankofengland.co.uk Bank of England and London School of Economics. Email: jamie.coen@bankofengland.co.uk Bank of England. Email: bill.francis@bankofengland.co.uk Bank of England and University College London. Email: may.rostom@bankofengland.co.uk 1 Abstract This paper examines the application of a suite of machine learning methods looking to predict credit union failure in the UK. The aim is to support the supervisory review process by understanding the benefits and limitations of machine learning as an early warning system for identifying credit union failures. A set of indicators on a firms financial health combined with macroeconomic variables such as growth and inflation are used as the feature set in the analysis. The report is divided as follows: (i) Chapter 1 introduces the classification problem and the feature set. (ii) Chapter 2 explores the pre-processing steps in preparing the data for training (model fit), validation (calibration) and testing (evaluation). (iii) Chapter 3 analyses the performance of nine classification algorithms, which includes, linear discriminant analysis, neural networks, naive Bayes, random forests and support vector machines. Performance is set out according to some criterion applied to the testing data, hence out-of-sample evaluation. (iv) Chapter 4 takes the best performing subset of these methods forward to perform cross validation, whilst covering important concepts such as the bias-variance trade-off, optimal model complexity and scalability.(v) Chapter 5 explores feature importance and (iv) Chapter 6 provides a conclusion and concise model summary. 2 Introduction Credit union failures are not uncommon. There have been 78 occurrences since 2002 as recorded in the data set utilised in this paper, equating to an average of 5/year. Credit union business models are inherently risky, with some offering products and services to a homogenous group of clients or having large exposures relative to their overall balance sheet size. This could make it hard for them to diversify their risks compared with larger banks. A credit union failure is where an institution is referred to the Financial Services Compensation Scheme (FSCS) for depositor pay-out. The data consists of a set of firm level financial indicators compiled from regulatory returns and macroeconomic variables covering the UK for the period 2002 to 2018 (Coen and Rostom 2018). The feature set includes firm measures of capital, liquidity, non-performing loans and balance sheet size. This is combined with macroeconomic indicators on unemployment (regionally), inflation (CPI) and economic growth (GDP, Quarterly). The response variable is a credit union DEFAULT, which is a Boolean indicator that takes the value 1 if the credit union defaults and 0 otherwise. Table 1 shows the summary statistics for the predictors below. Table 1: Summary Statistics of Credit Union Data Statistic N Mean St. Dev. Pctl(25) Pctl(75) Return on Assets (%) 7,961 1.5 2.9 0.5 3.0 Arrears rate (%) 7,961 8.5 8.6 2.6 11.0 Loan-deposit ratio (%) 7,929 64.0 27.0 43.0 83.0 Risk-adjusted capital ratio (%) 7,961 9.0 6.9 4.8 12.0 Provisions-loans ratio (%) 7,961 8.0 7.2 3.1 9.2 Log assets(£) 7,961 13.0 1.7 12.0 14.0 Unsecured loans to assets (%) 7,961 40.0 23.0 22.0 56.0 Capital ratio (%) 7,936 71.0 44.0 40.0 93.0 Arrears &lt; 12 Months (%) 7,961 3.9 3.5 1.6 3.9 Arrears &gt; 12 Months (%) 7,961 3.2 5.0 0 3.2 Inflation (%, CPI) 7,961 2.1 1.0 1.4 2.7 GDP Growth (Quarterly, %) 7,961 1.8 1.6 1.6 2.5 Bank Rate (%) 7,961 2.2 2.0 0.5 4.0 Regional Unemployment Rate (%) 7,533 6.2 1.6 4.9 7.3 The dataset is heavily imbalanced, with 81 DEFAULT occurrences in the data and 7880 active credit union observations at the date of reporting. The imbalanced nature of the dataset will be explored more in Chapter 3. In order to develop a better understanding of the data, a boxplot of each of the predictors are compared against DEFAULT rates are shown in Chart 1. Chart 1 provides some useful insights into the relationship between DEFAULT and the predictors in the data. It is clear that DEFAULT rates amongst credit unions are generally higher when total assets and return on assets (roa) are lower. Similarly, DEFAULT rates appear to be lower amongst entities with overall lower loan to deposit ratios, percentage of loans in arrears, unsecured assets and regional unemployment. These are all intuitive results, for example it is expected that the firm size to be inversely correlated with DEFAULT rates, since larger credit unions are likely to have a more diverse balance sheet and can withstand economic shocks. To understand how these variables interact with one another, Chart 2 shows a correlation plot of the features. Chart 2 suggests there is some correlation between features, most notably unsecured assets and the loan to deposit ratio, percentage loans in arrears and loan provisions and some pronounced correlation amongst the macroeconomic indicators. A consideration here is to remove features that are highly correlated, particularly to limit issues with dimensionality and scalability of the algorithms; however this is not applied here. 3 Data Preparation In this chapter the data is prepared, which includes pre-processing and ‘rebalancing’ the DEFAULT class. First the training set and test set are generated on the basis of random sampling, with the split set to be 70% training and 30% test (p = 0.7). A relatively large proportion of the data is kept back for testing, which is due to the class imbalances, requiring a larger portion of the data for evaluation purposes. The steps taken to prepare the training and test sets for the model are shown below, with the results displayed in Table 2. variables &lt;- c(&quot;Year&quot;, &quot;DEFAULT&quot;, &quot;roa&quot;, &quot;pctarrears&quot;, &quot;loandeposit&quot;, &quot;simplecapitalratio&quot;, &quot;provlns&quot;, &quot;logassets&quot;, &quot;unsecassets&quot;, &quot;CapitalRatio&quot;, &quot;arrears312&quot;, &quot;arrearsg12&quot;, &quot;CPI&quot;, &quot;GDP.Growth.Quarter.Last.Year&quot;, &quot;Base.Rate&quot;, &quot;Regunemployment&quot;) DataML &lt;- dataClean[, variables, with = FALSE] DataML &lt;- na.omit(DataML) # prepare data for ML analysis DataML &lt;- PrepForRecipe(as.data.frame(DataML)) nrow_DataML &lt;- tibble::tibble(`Before Split` = nrow(DataML)) set.seed(12345) train_test_split &lt;- rsample::initial_split(DataML, prop = 0.7) train &lt;- rsample::training(train_test_split) # create training set test &lt;- rsample::testing(train_test_split) # create test set nrow_train_data &lt;- tibble::tibble(`Training set` = nrow(train)) nrow_test_data &lt;- tibble::tibble(`Test set` = nrow(test)) table &lt;- cbind(nrow_DataML, nrow_train_data, nrow_test_data) kableExtra::kable(table, caption = &quot;Training and test sets&quot;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;), full_width = F, position = &quot;left&quot;) Table 3.1: Training and test sets Before Split Training set Test set 7478 5235 2243 As mentioned, the credit union data is heavily imbalanced. To rebalance the data, up-sampling is applied in addition to data pre-processing, which replicates rows of a data set to make the occurrence of levels in a specific class more balanced. Down-sampling was also considered, whereby the majority class is sampled and removed from the data. There are more sophisticated rebalancing methods, such as generating synthetic data from the minority class (SMOTE) to increase its cardinality, which will be explored in Chapter 4. Pre-processing refers to the transformations applied to our data before running the models. The features undergo the following steps; step_center - normalizes (numeric) variables to have mean zero step_scale - normalizes variables to have standard deviation one step_dummy - creates dummy variables from factor levels step_upsample - rebalances the data by up-sampling the DEFAULT class observations. The implementation of this can be found in the Annex with the results shown in Table 3 (Barter 2019). #--------------------------------------------------------------------------------------- # Create &#39;recipe&#39; for pre-processing input data, and apply to one example split of data -------- simple_recipe &lt;- function(dataset) { recipes::recipe(DEFAULT ~ ., data = dataset) %&gt;% recipes::step_center(all_numeric()) %&gt;% recipes::step_scale(all_numeric()) %&gt;% recipes::step_dummy(all_nominal(), -DEFAULT) %&gt;% recipes::step_upsample(DEFAULT, ratio = 0.5) # this step rebalances the data } train_recipe &lt;- recipes::prep(simple_recipe(train), training = train, retain = TRUE) # prepare data model, apply processing steps train_data &lt;- train_recipe %&gt;% juice # generate training set test_data &lt;- recipes::bake(train_recipe, new_data = test) # generate test set table &lt;- rbind(c(CountDefaultsTrain = sum(train_data$DEFAULT == 1), CountNonDefaultsTrain = sum(train_data$DEFAULT == 0)), c(CountDefaultsTest = sum(test_data$DEFAULT == 1), CountNonDefaultsTest = sum(test_data$DEFAULT == 0))) rownames(table) &lt;- c(&quot;training data&quot;, &quot;test data&quot;) Table 3: Summary Statistics of prep processed training data Statistic N Mean St. Dev. Min Pctl(25) Pctl(75) Max roa 7,765 -0.4 1.4 -4.3 -0.9 0.4 3.0 pctarrears 7,765 0.3 1.2 -1.0 -0.6 0.9 4.5 loandeposit 7,765 0.3 1.1 -2.1 -0.5 1.0 2.8 simplecapitalratio 7,765 -0.2 1.3 -2.5 -0.9 0.3 4.2 provlns 7,765 0.3 1.2 -1.0 -0.6 0.6 4.7 logassets 7,765 -0.2 1.0 -1.9 -0.9 0.4 2.5 unsecassets 7,765 0.2 1.1 -1.6 -0.6 1.1 2.3 CapitalRatio 7,765 -0.1 1.0 -1.5 -0.9 0.3 5.3 arrears312 7,765 0.3 1.3 -1.1 -0.4 0.4 4.9 arrearsg12 7,765 0.3 1.4 -0.6 -0.6 0.1 5.8 CPI 7,765 0.1 1.1 -2.0 -0.8 0.6 2.3 GDP.Growth.Quarter.Last.Year 7,765 -0.01 1.0 -3.6 -0.1 0.4 0.9 Base.Rate 7,765 0.05 1.0 -1.0 -0.9 1.1 1.6 Regunemployment 7,765 0.1 1.0 -1.8 -0.6 0.7 3.0 Input pre-processing is an important aspect, which may crucially affect a models predictive performance. Several techniques, namely those that combine inputs such as artificial neural networks are highly sensitive to different features. This is addressed via feature standardisation (as above), which allows all features to be compared easily and applied to different model types (Chakraborty and Joseph 2017). The data is now ready to apply the learning algorithms, which is explored in Chapter 3. 4 Model Fitting 4.1 Measuring Performance In this chapter a set of popular models within the machine learning toolbox are considered. The objective here is to fit an array of models the data (training set) and evaluate each models performance using a set of accuracy measures. The models considered include logistic regression (baseline), naive Bayes, k-nearest neighbours (KNN), tree algorithms, artificial neural networks (ANN), linear discriminant analysis (LDA) and support vector machines (SVM). Model performance is considered on the basis that a there is an overall relative preference between a Type I error (misclassifying a default, DEFAULT = 1) and a Type II error (misclassifying a healthy firm, DEFAULT = 0) (Coen and Rostom 2018). Five accuracy measures are used to assess model performance (Lantz 2019); Accuracy (poor measure for imbalanced data) - (TP + TN)/(P + N) Balanced accuracy - (TP/P + TN/N)/2 Sensitivity (true positive rate) - TP/(TP + FN) Specificity (true negative rate) - TN/(TN + FP) Receiver Operating Characteristic (ROC) - sensitivity vs (1 − specificity) Here there is a trade off from a supervisory perspective in which supervisors are concerned with the elements of the confusion matrix that are of high importance. Firstly, missing credit union defaults, otherwise known as false negatives. Secondly, raising false alarms on viable credit unions and wasting supervisory resources. The first is captured in the Specificity of the model, a high Specificity means the model is correctly identifying DEFAULT classes. The second is captured in the Sensitivity, with a lower Sensitivity meaning there are a lot of false flags in the predictions. The ROC curve is a useful performance measure as the inherent computation shows the trade-off between the benefits. These measures will be considered throughout this analysis to understand how the models are performing relative to one-another. 4.2 Baseline Logistic Model First a logistic model is applied to the data to act as a baseline for performance evaluation. Logistic models are used to predict the probability that an observation falls into one of two categories; DEFAULT = 1 or DEFAULT = 0 based on a set of predictors (features). The instantiation of the logistic model is given in the Annex. #--------------------------------------------------------------------------------------- # fit logistic model using parsnip logistic_reg function, fit using training data logisticModel &lt;- parsnip::logistic_reg(mode = &quot;classification&quot;, penalty = &quot;glm&quot;) %&gt;% parsnip::set_engine(&quot;glm&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # obtain model results using function (see annex for details) applying to the test data logisticModelResults &lt;- modelResults(logisticModel, testData = test_data, &quot;logisticModel&quot;) Table 3.1 shows the model performance statistics. Note the accuracy is 91%; however this is not a good measure due to the class imbalances in the data. The balanced accuracy is 71%, whilst the specificity is only around 50%. The interpretation of this is that the logistic model is predicting around half of the DEFAULT cases correctly, whilst there are a number of false positives. Remember a supervisor has a relative preference of specificity, whilst minimising the number of false flags. All accuracy measures are computed using the result’s from the confusion matrix in Table 3.1 Table 4.1: Model performance for logistic regression .metric .estimator .estimate type accuracy binary 0.90 logisticModel sens binary 0.90 logisticModel spec binary 0.48 logisticModel bal_accuracy binary 0.69 logisticModel precision binary 0.99 logisticModel roc_auc binary 0.79 logisticModel Table 4.1: Confusion matrix Prediction Truth 0 1 0 2009 211 1 12 11 Chart 3 shows the ROC curve for the logistic regression baseline model, which shows the trade-off between sensitivity and specificity. Classifiers that give curves closer to the top-left corner indicate a better performance (Lantz 2019). 4.3 Machine Learning Classifiers In this section an array of classifiers are explored. Each model is fitted to the data using a ‘training’ sample. The ‘testing’ sample is used to evaluate each model’s performance in predicting credit union failures. The model fits can be seen in the Annex. Once the models have been fitted using the training data, each model’s performance can be evaluated according to the criterion outlined in Chapter 3, when applied to unseen data (out-of-sample). The results are shown in Table 3.2. #--------------------------------------------------------------------------------------- # fit random forst model using parsnip rand_forest function, fit using training data randomForest &lt;- parsnip::rand_forest(mtry = 10, trees = 150, min_n = 6, mode = &quot;classification&quot;) %&gt;% parsnip::set_engine(&quot;randomForest&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # run boosted trees alogrithm and test performance boostedTree &lt;- parsnip::boost_tree(mtry = 10, trees = 100, min_n = 6, mode = &quot;classification&quot;) %&gt;% parsnip::set_engine(&quot;xgboost&quot;, importance = &quot;impurity&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # run decision tree and test model performance decisionTree &lt;- parsnip::decision_tree(mode = &quot;classification&quot;, cost_complexity = 10, min_n = 6) %&gt;% parsnip::set_engine(&quot;C5.0&quot;, importance = &quot;impurity&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # run KNN model KNNmodel &lt;- parsnip::nearest_neighbor(mode = &quot;classification&quot;, neighbors = 10) %&gt;% parsnip::set_engine(&quot;kknn&quot;, importance = &quot;impurity&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # run SVM models SVMmodel &lt;- parsnip::svm_rbf(mode = &quot;classification&quot;, rbf_sigma = 0.2) %&gt;% parsnip::set_engine(&quot;kernlab&quot;, importance = &quot;impurity&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # run LDA models LDAModel &lt;- MASS::lda(DEFAULT ~ ., data = train_data) # run naive bayes models NaiveBayesModel &lt;- e1071::naiveBayes(DEFAULT ~ ., data = train_data) # run neural network model with tuning gride tuning.grid &lt;- expand.grid(.size = 1:6, .decay = 0, .bag = FALSE) NeuralNetModel.tuned &lt;- caret::train(DEFAULT ~ ., data = train_data, method = &quot;avNNet&quot;, trace = FALSE, tuneGrid = tuning.grid, linout = TRUE, repeats = 1) Table 4.2: Model performance statistics model accuracy bal_accuracy roc_auc sens spec boostedTree 0.98 0.53 0.80 1.00 0.06 decisionTree 0.97 0.52 0.63 0.98 0.06 KNN 0.96 0.54 0.37 0.98 0.09 LDA 0.91 0.71 0.71 0.92 0.50 logisticModel 0.91 0.71 0.85 0.91 0.50 NaiveBayes 0.88 0.73 0.71 0.89 0.56 NeuralNetwork 0.92 0.67 0.71 0.93 0.41 RandomForest 0.98 0.50 0.80 1.00 0.00 SVM 0.98 0.51 0.80 0.99 0.03 Table 3.2 shows that the LDA, logistic, Naive Bayes and ANN models perform the best as evaluated using balanced accuracy. The Naive Bayes classifier outperforms the others in terms of specificity (56%), whilst the other three aforementioned models perform well. Chart 4 shows the ROC curve for the models listed above. The steps to build a machine learning system include training, validation and testing. The next section of this chapter explores model validation, which is the process of calibrating the models to maximise out-of-sample performance. This may either mean the variation of a model’s degrees of freedom, like the number of nodes and layers in a neural network, tuning the hyper-parameters using a meta-algorithm or changing the rebalancing method. NOTE: The data training data has not been rebalanced (as explained in Chapter 2) here, meaning only the standardisation pre-processing steps have been applied the data. The code in the Annex shows the implementation of four rebalancing methods, using the LDA model. Here the number of folds is set to k = 10. The performance metrics are shown in Table 3.3. method = c(&quot;up&quot;, &quot;down&quot;, &quot;smote&quot;, &quot;rose&quot;) ModelPerformance &lt;- lapply(X = c(&quot;up&quot;, &quot;down&quot;, &quot;smote&quot;, &quot;rose&quot;), FUN = trainControlSample, method = &quot;nb&quot;, resamplingmethod = &quot;boot&quot;, traindata = train_data, testdata = test_data, numberFolds = 10) data &lt;- ModelPerformance %&gt;% purrr::map(1) %&gt;% purrr::invoke(rbind, .) rebalancePerformance &lt;- tidyr::spread(data, key = Meausure, value = Position) Table 4.3: Model performance statistics for different rebalancing methods samplingMethod modeltype Balanced Accuracy Recall Sensitivity Specificity down nb 0.78 0.90 0.90 0.66 rose nb 0.72 0.91 0.91 0.53 smote nb 0.72 0.92 0.92 0.53 up nb 0.76 0.90 0.90 0.62 Using balanced accuracy, sensitivity and specificity as the performance measures, it appears as though down-sampling and up-sampling are comparable. Up-sampling is chosen as the best rebalancing method, which consists replicating some points from the minority class. The same result holds for the logistic, naive Bayes and neural networks (analysis omitted). The next step is to understand which of the four models performs best when subjected cross validation. Here the number of folds is set to k = 5. The cross-validation parameters are set as follows; The resampling method: method = boot (see annex) Number of folds: number = 5, Rebalancing method: sampling = up Each model is evaluated, using resampling, evaluating each instance to obtain an overall accuracy estimate. The implementation of cross-validation is shown in the Annex with the results displayed in Table 3.4. Table 4.4: Model performance statistics, cross validated modeltype Balanced Accuracy Recall Sensitivity Specificity lda 0.72 0.83 0.83 0.62 nb 0.70 0.89 0.89 0.52 nnet 0.72 0.92 0.92 0.52 plr 0.70 0.82 0.82 0.59 Table 4.5: LDA Confusion matrix LDA Prediction Reference 0 1 0 1828 386 1 11 18 Table 4.5: Logistic case Logistic Prediction Reference 0 1 0 1813 401 1 12 17 Naive Bayes Prediction Reference 0 1 0 1966 248 1 14 15 Neural Networks Prediction Reference 0 1 0 2043 171 1 14 15 Looking at the performance measures, LDA outperforms the other models at 72% balanced accuracy and 62% specificity, which is comparable to the logistic model. The LDA model is successfully classifying DEFAULT cases around 62% of the time, and is raising few ‘false flags’ within the test set. The LDA correctly classifies 18 out of 29 DEFAULT observations; however the model raises 386 false positives and performs the worst as measured by sensitivity. The LDA sensitivity performance is much lower than the other classifiers, which is a counter to the supervisory preference of minimising the number of false flags of viable entities. The ANN performs best in terms of sensitivity (92%). The model correctly identifying 15 out of 29 DEFAULT observations, whilst raising significantly less false positives (171). The naive Bayes performs comparably with ANN. Given the overall specificity performance comparability between the LDA and neural networks, the neural network algorithm is considered preferable on the basis it produces significantly less false positives. Cross-validation is performed on the ANN by applying a set of permutations to the model’s degrees of freedom in an attempt to enhance predictive performance. This is explored in the next Chapter 4. A simplified visual representation of the neural network (hidden = 3) can be seen below in Chart 5. 5 Cross Validation 5.1 Model Tuning One way to improve a model is cross-validation (cv), which applies the algorithms over different sets of parameters or degrees of freedom, then comparing the model performance over these sets. K-fold cv is applied to the neural network, whilst varying the degrees of freedom in the model. The model is computed over a tuning grid consisting of the size (units in hidden layer) and decay (regularization parameter to avoid over-fitting). The set within the tuning grid is selected based on the best out of sample performance, which can be seen below; Chart 4 shows the results of the cv exercise. As measured by overall accuracy, the model performance is maximised for size = 19 and decay = 0.1. 5.2 Bias-variance trade off When it comes to machine learning, there is a trade of between model performance and cost. Basic models with few degrees of freedom (logistic case) are often simple to calculate (low cost); however may lead to poorer model fits and performance (e.g. under-fitting, when there is a non-linear relationship). On the other hand, sophisticated models such as ANN can provide more accurate fits, as demonstrated above but are computationally intensive (cv ANN &gt; 10mins to compute). In addition, complex models with a large number of parameters can lead to overfitting or be subject to a lot of variance (bias-variance trade off) (Lantz 2019). The cv process, as demonstrated in Section 2 can help calibrate a model’s fit, which in turn can improve predictive performance. 5.3 Overfitting ANN have a lot of freedom; however a lack of control over the learning process can lead to overfitting. This is a situation when the neural network is so closely fitted to the training set that it is difficult make accurate out of sample predictions on previously unseen data. When a given method yields a small training error (low cost) but a large test error, then this is a sign of overfitting the data (Gutierrez 2015). One method to reduce overfitting is regularisation, which introduces a loss function that penalises the model for being too complex (Brownlee 2015). 5.4 Scalability Thus far, only the accuracy of the model has been considered. The scalability of the machine learning algorithm is of importance when implementing such solutions. ANN are computationally intensive and slow to train. The credit union data consists of ~112,000 observations, which is not considered a large-scale database; however even with this modest dataset, cv and feature analysis was time consuming (using parallelisation doParallel). It is possible run these algorithms on a local machine; however given the ever growing set of records, one day this may become a problem. Feature selection can be helpful to reduce dimensionality, which selects only the most relevant features, Chapter 5. The library ‘sparklyr’ is a potential solution to this, which is a distributed processing system used for big data workloads. The implementation of this would look something like the following; sc &lt;- spark_connect(master = &quot;local&quot;) DataML &lt;- sdf_copy_to(sc, DataML, name = &quot;mtcars_tbl&quot;, overwrite = TRUE) partitions &lt;- DataML %&gt;% sdf_random_split(training = 0.7, test = 0.3, seed = 1111) training &lt;- partitions$training test &lt;- partitions$test logistic.model &lt;- training %&gt;% ml_logistic_regression(DEFAULT ~ .) pred &lt;- ml_predict(logistic.model, test) ml_binary_classification_evaluator(pred) 6 Feature importance This section explores the overall feature importance in the model. Neural networks and other machine learning models are often referred to as ‘black box’ models. This is because these models lack explicability and interpretability, since the layers and operations applied in the algorithms are not visible or easy to understand. This is sometimes hard look past, especially in the case of firm supervision. The relative importance of individual features can give end-users a better understanding of a model’s output. It is important to understand the source of the classifications, namely the default classes DEFAULT = 1, in the context of the data. One approach to understanding the contribution of a feature in a model is to observe the amount by which the inclusion or exclusion of a feature changes the performance. Chart 5 shows the relative importance of the features, based on the ANN fitted in Section 3. importance &lt;- caret::varImp(nnetFit, scale = FALSE) # summarize importance print(importance) nnet variable importance Overall unsecassets 10.12 Regunemployment 9.75 loandeposit 8.56 logassets 8.19 CY_30E 7.77 arrearsg12 7.69 roa 7.66 pctarrears 7.65 CPI 6.76 provlns 6.58 simplecapitalratio 6.07 GDP.Growth.Quarter.Last.Year 5.62 Base.Rate 4.05 arrears312 3.52 Chart 5 shows that unsecured assets, unemployment, loan to deposit ratio and log assets are the most important features in the model. This illustrates the pronounced effects of unemployment and the importance considering macroeconomic as well as microeconomic factors. Note that the number of features doesn’t have to be the same as the explanatory predictors. Adding an interactional or functional set of features can capture other complexities in the data. By doing so it may be possible to enhance predictive performance by enriching the dataset. In some cases the data is not separable when presented in its raw form; however when re-engineered, features can help distinguish between the classes and, so, improve the classifier accuracy. Recursive Feature Elimination (RFE) is a feature selection method that fits a model and removes the weakest feature. The method uses Random Forest algorithm to evaluate the model, which is configured to explore all possible subsets of the attributes (Brownlee 2014). The results of the RFE are shown below in Chart 6. # define the control using a random forest selection function control &lt;- caret::rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10) # run the RFE algorithm results &lt;- caret::rfe(DEFAULT ~ ., data = train_data, sizes = c(1:8), rfeControl = control) # summarize the results print(results) Chart 6 implies that the optimal number of features is six. The most ‘important’ features in the data under RFE are displayed below. Note this is broadly in agreement with results displayed in Chart 5. [1] &quot;logassets&quot; &quot;roa&quot; &quot;simplecapitalratio&quot; &quot;Regunemployment&quot; [5] &quot;CY_30E&quot; &quot;unsecassets&quot; 7 Conclusion This paper introduced an array of supervised machine learning classification algorithms and explored the effectiveness of each in predicting credit union defaults. The classifiers included artificial neural networks, support vector machines, naive Bayes, linear discriminant analysis, support vector machines and tree algorithms, which were all measured against the baseline fit in the form of a logistic model. The baseline logistic, naive Bayes, LDA and neural networks performed best as measured by balanced accuracy and specificity, which helped capture the supervisory preference between Type I error and a Type II error. The four aforementioned classifiers were taken forward to undergo model calibration, which consisted of changing the rebalancing method, changing the resampling method and varying the model’s degrees of freedom. It resulted that there ANN performed best, on the basis its Type I error was comparable to the other models, whilst its Type II error outperformed the other classifiers significantly. The neural network itself poses some challenges, namely overfitting and the interpretability of the results (‘black box methods’). This analysis highlighted that there is no ‘fixed’ machine learning model, but a large array of model types, each of which consist of their own degrees of freedom, sets of hyper-parameters and different interpretations of ‘success’ as captured by the performance metrics. The ‘success’ is a combination of the right model ingredients, but also depends on the knowledge and understanding of the end user (supervisory teams), whilst keeping one eye on the end goal, that is to flag firms that are at risk of default, whilst minimising the overall number of false flags of perfectly viable entities. It follows that the ANN model is the best when considering these trade-offs; however it could be limited in its application due to its lack of interpretability and scalability due to computational challenges. In light of this, other classification algorithms may be preferred; however, in this paper, artificial neural networks demonstrated its ability to uncover nuanced relations between variables and account for heterogeneity amongst enitities. 8 References Barter, Rebecca. 2019. “Using the Recipes Package for Easy Pre-Processing.” http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/. Brownlee, Jason. 2014. “Feature Selection with the Caret R Package.” https://machinelearningmastery.com/feature-selection-with-the-caret-r-package/. ———. 2015. “How to Avoid Overfitting in Deep Learning Neural Networks.” https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/. Chakraborty, Chiranjit, and Andreas Joseph. 2017. “Machine Learning at Central Banks.” https://www.bankofengland.co.uk/-/media/boe/files/working-paper/2017/machine-learning-at-central-banks.pdf?la=en&amp;hash=EF5C4AC6E7D7BDC1D68A4BD865EEF3D7EE5D7806. Coen, Francis, and Rostom. 2018. “The Determinants of Credit Union Failure in the United Kingdom.” https://www.bankofengland.co.uk/-/media/boe/files/working-paper/2018/the-determinants-of-credit-union-failure-in-the-uk-update.pdf?la=en&amp;hash=4D30F860543FE038FADB8655FD70559C9F6D348F. Gutierrez, Daniel. 2015. “Machine Learning and Data Science: An Introduction to Statistical Learning.” https://www.oreilly.com/library/view/machine-learning-and/9781634620987/. Lantz, Brett. 2019. “Machine Learning with R Second Edition.” VitalSource Bookshelf. https://online.vitalsource.com/. 9 Appendix 9.1 Extensions 9.1.1 Multi-step forecast This analysis was conducted using a static approach, i.e. a model that is looking to predict credit union defaults during in a single reporting period. Based on a regulatory returns submitted to the Bank of England, firm information could be used to help classifying entities into one of two categories, DEFAULT and NON DEFAULT. The firms falling into the first category could then be triaged and put forward for further investigation. This is known as a One-Step Forecast, which is where the next time step (t+1) is predicted. It is possible to extend the machine learning approach to model failure using a multi-period model since there is an appetite from supervisors to anticipate failure regardless time period. This could be incorporated into the model by including time dummy variables (year effects) in the feature set. This is known as a Multi-Step Forecast, which is where two or more future time steps (t+1, t+2, t+3, …, t+n) are to be predicted. It would be expected that over a given time horizon over which we predict failure, model performance would decline; however this could provide supervisors with early warning indicators before it is too late, thus allowing for pre-emptive action to be taken. Under the multip step approach, the additional set of features would look something like follows; DEFAULT failurel1 failurel2 failurel3 failurel4 1: 0 0 0 0 1 2: 1 0 0 1 0 3: 0 0 0 0 0 4: 0 0 0 0 0 5: 0 0 0 0 0 6: 0 0 0 0 0 9.1.2 Model tuning The models explored in this exercise peform relatively in predicting credit union defaults when accounting for supervisory preferences between Type I and Type II erros. Model performance could be further enhanced via the application of some of the following techniques; Regularisation: This is a smoothing method that introduces a penalty term to the error function acting on non-zero, especially large, parameter values. Its effect is the reduction of the complexity, or the smoothening, of a model. This is useful when faced with an over-fitting model but also for feature selection, which can sometimes be problematic in neural networks. Bagging: A popular method for improving an estimator’s properties. The idea is that, when aggregating (averaging) the outputs of many estimators, one reduces the variance of the full model. When over-fitting is an issue, as for tree models, bagging is likely to improve performance. Boosting: Describes a set of methods which transform, or boost, a weak learner into a strong one (model). This means that one iteratively applies models to the data which individually have weak generalisation properties, but the final ensemble of models generalises well. Simulated data, which involves creating synthetic data of the minority class (DEFAULT = 1) to help rebalance the minority class. This could be achieved with the help of supervisor’s specialist knowledge of the entities themselves and the associated risks. The result could be better out of sample predictive performance as the model has ‘seen’ more instances of what a DEFAULT looks like and is therefore more likely to be able to identify future instances. 9.2 Source Code All code can be found on the git repository here CreditUnionFailureML. The code snippits below outline the steps taken in conducting this analysis, from data preperation, model fitting, cross-validation, performance evaluation and testing. Below is the list of helper functions created to support the analysis. # helper funs #&#39; @title BOE colour pallete used for charting boe_cols &lt;- c( red = &quot;#A51140&quot;, black = &quot;#1E1E1E&quot;, stone = &quot;#CAC0B6&quot;, dark_teal = &quot;#005E6E&quot;, dark_blue = &quot;#002A42&quot;, plum = &quot;#752864&quot;, mid_blue = &quot;#165788&quot;, maroon = &quot;#6C0721&quot;, purple = &quot;#4E3780&quot;, grey = &quot;#999999&quot;, green = &quot;#006663&quot;, orange = &quot;#D55E00&quot;, orange2 = &quot;#E69F00&quot;, blue = &quot;#56B4E9&quot;) MakeROCPlot &lt;- function(data, modelType, title){ ModelAccuracy %&gt;% dplyr:: filter(imbalanceMethod == paste0(modelType)) %&gt;% yardstick:: roc_curve(truth, .pred_1) %&gt;% ggplot2:: ggplot(aes(x = 1 - specificity, y = sensitivity, fill = paste0(modelType), group = &quot;1&quot;)) + ggplot2:: geom_path(aes( colour = unname(boe_cols)[1]), show.legend = F, lwd =2) + ggplot2:: geom_abline(lty = 4, colour = unname(boe_cols)[3], show.legend = FALSE, lwd = 2) + ggplot2:: scale_colour_manual(values = rep(unname(boe_cols),1)) + ggplot2:: coord_equal() + ggplot2:: theme_minimal(base_size = 16) + ggplot2:: labs(subtitle = paste0(title)) } PrepForRecipe &lt;- function(df, ...){ for(i in 2:nrow(df)) { if((nrow(df) %% i) == 0) { i break() } } df &lt;- df[,which(!colnames(df) %in% c(&quot;FRN&quot;,&quot;FirmName&quot;, &quot;Year&quot;))] df$DEFAULT &lt;- as.factor(df$DEFAULT) return(df) } simple_recipe &lt;- function(df){ recipe(Raise ~ ., data = df) %&gt;% step_center(all_numeric()) %&gt;% step_scale(all_numeric()) %&gt;% step_dummy(all_nominal(),-Raise) %&gt;% step_upsample(Raise, ratio = 0.5) } yncount &lt;- function(df){ y &lt;- countraise(df, &quot;Yes&quot;) print(y) n &lt;- countraise(df, &quot;No&quot;) print(n) } ##Function to obtain the perdicted class from the model and test data along with #Probabilty scores. predictAndProb &lt;- function(model, testData,imbalanceMethod){ pred_raw = predict(model, new_data = testData, type = &quot;raw&quot;) pred_probs = predict(model, new_data = testData,type = &quot;prob&quot;) pred_class = predict(model, new_data = testData, type = &quot;class&quot;) #Building the predictions of the model and the data with probablity predictions = tibble:: tibble(truth = testData$DEFAULT, predicted = pred_class$.pred_class) %&gt;% cbind(pred_probs) %&gt;% dplyr:: mutate(imbalanceMethod = imbalanceMethod) return(as_tibble(predictions)) } ##Code that extracts performance metrics after running the model, this is a single #model not one that is cross validated. modelResults &lt;- function(model, testData, type){ modelAccuracy &lt;- predictAndProb(model, testData, type) confMatrix &lt;- yardstick:: conf_mat(modelAccuracy, truth, predicted) confMatrixStats &lt;- summary(confMatrix) auc &lt;- yardstick:: roc_auc(modelAccuracy, truth, .pred_1) modelStats &lt;- rbind(confMatrixStats, auc) %&gt;% dplyr:: mutate(type = type) plot &lt;- modelAccuracy %&gt;% yardstick:: roc_curve(truth, .pred_1) %&gt;% autoplot return( list(confMatrix = confMatrix, modelStats = modelStats, modelAccuracy = modelAccuracy, plot = plot) ) } #------------------------------------------------------------------------------ trainControlSample &lt;- function(traindata, testdata, samplingMethod, method, resamplingmethod, numberFolds = 10, repeats = 5){ ctrl &lt;- trainControl(method = paste0(resamplingmethod), number = numberFolds, verboseIter = FALSE, repeats = repeats, sampling = paste0(samplingMethod)) set.seed(42) model_trained &lt;- caret::train(DEFAULT ~ ., data = traindata, method = paste0(method), #preProcess = c(&quot;scale&quot;, &quot;center&quot;), trControl = ctrl) finalPreds &lt;- data.frame(actual = testdata$DEFAULT, predict(model_trained, newdata = testdata, type = &quot;prob&quot;)) finalPreds$predict &lt;- as.factor(ifelse(finalPreds$X1 &gt; 0.5, 1, 0)) cm_over &lt;- confusionMatrix(finalPreds$predict, testdata$DEFAULT) Results &lt;- data.table:: data.table(cm_over$byClass) colnames(Results)[1] = &quot;Position&quot; Results &lt;- Results[, `:=`(Meausure = names(cm_over$byClass), resamplingmethod = paste0(resamplingmethod), samplingMethod = paste0(samplingMethod), modeltype = paste0(method))] return(list(Results, cm_over$table)) } 9.2.1 Chapter 2 The code below shows steps taken in preparing the data for the analysis as discussed in Chapter 2. This includes reading the data, transforming the data to be structured for a machine learning problem and outputting the summary statistics. library(tidyverse) library(data.table) library(stargazer) basePath = file.path(&quot;//researchhub/files/May/Credit Union Failure/SRDDfolder&quot;) # source data from files dataRaw = data.table::fread(file.path(basePath, &quot;appdatawin.csv&quot;), stringsAsFactors = FALSE) # select columns to keep cols = colnames(dataRaw)[which(!grepl(pattern = &quot;failure&quot;, x = colnames(dataRaw), ignore.case = TRUE))] # prepare data dataClean = dataRaw[, match(cols, colnames(dataRaw)), with = FALSE][, `:=`(FirmName, Firm.Name.x)][, -&quot;Firm.Name.x&quot;] variables &lt;- c(&quot;roa&quot;, &quot;pctarrears&quot;, &quot;loandeposit&quot;, &quot;simplecapitalratio&quot;, &quot;provlns&quot;, &quot;logassets&quot;, &quot;unsecassets&quot;, &quot;CY_30E&quot;, &quot;arrears312&quot;, &quot;arrearsg12&quot;, &quot;CPI&quot;, &quot;GDP.Growth.Quarter.Last.Year&quot;, &quot;Base.Rate&quot;, &quot;Regunemployment&quot;) y &lt;- dataClean[, variables, with = FALSE] colnames(y) &lt;- c(&quot;Return on Assets (%)&quot;, &quot;Arrears rate (%)&quot;, &quot;Loan-deposit ratio (%)&quot;, &quot;Risk-adjusted capital ratio (%)&quot;, &quot;Provisions-loans ratio (%)&quot;, paste0(&quot;Log assets(&quot;, &quot;£&quot;, &quot;)&quot;), &quot;Unsecured loans to assets (%)&quot;, &quot;Capital ratio (%)&quot;, &quot;Arrears &lt; 12 Months (%)&quot;, &quot;Arrears &gt; 12 Months (%)&quot;, &quot;Inflation (%, CPI)&quot;, &quot;GDP Growth (Quarterly, %)&quot;, &quot;Bank Rate (%)&quot;, &quot;Regional Unemployment Rate (%)&quot;) stargazer::stargazer(y, title = &quot;Summary Statistics&quot;, type = &quot;html&quot;, out = &quot;./Outputs/summarystats.html&quot;, digits = 1) #-------------------------------------------------------------------------------------------------------------------------- The code below shows steps taken in preparing the data for machine learning classifiers as discussed in Chapter 2. This includes creating the training and test splits. variables &lt;- c(&quot;FRN&quot;, &quot;FirmName&quot;, &quot;Year&quot;, &quot;DEFAULT&quot;, &quot;roa&quot;, &quot;pctarrears&quot;, &quot;loandeposit&quot;, &quot;simplecapitalratio&quot;, &quot;provlns&quot;, &quot;logassets&quot;, &quot;unsecassets&quot;, &quot;CapitalRatio&quot;, &quot;arrears312&quot;, &quot;arrearsg12&quot;, &quot;CPI&quot;, &quot;GDP.Growth.Quarter.Last.Year&quot;, &quot;Base.Rate&quot;, &quot;Regunemployment&quot;) DataML &lt;- dataClean[, variables, with = FALSE] DataML &lt;- na.omit(DataML) # prepare data for ML analysis DataML &lt;- PrepForRecipe(as.data.frame(DataML)) nrow_DataML &lt;- tibble::tibble(`Before Split` = nrow(DataML)) set.seed(12345) train_test_split &lt;- rsample::initial_split(DataML, prop = 0.7) train &lt;- rsample::training(train_test_split) # create training set test &lt;- rsample::testing(train_test_split) # create test set nrow_train_data &lt;- tibble::tibble(`Training set` = nrow(train)) nrow_test_data &lt;- tibble::tibble(`Test set` = nrow(test)) table &lt;- cbind(nrow_DataML, nrow_train_data, nrow_test_data) kableExtra::kable(table, caption = &quot;Training and test sets&quot;) %&gt;% kableExtra::kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;hover&quot;, &quot;condensed&quot;), full_width = F, position = &quot;left&quot;) The final step before the models are fitted is the prep-processing and standardisation of the data, which is shown below. 9.2.2 Chapter 3 The first step is to fit the models, using a set of classifiers identified in this paper as discussed in Chapter 3.3. The first is the logistic case, which is fitted as follows; #--------------------------------------------------------------------------------------- # fit logistic model using parsnip logistic_reg function, fit using training data logisticModel &lt;- parsnip::logistic_reg(mode = &quot;classification&quot;, penalty = &quot;glm&quot;) %&gt;% parsnip::set_engine(&quot;glm&quot;) %&gt;% parsnip::fit(DEFAULT ~ ., data = train_data) # obtain model results using function (see annex for details) applying to the test data logisticModelResults &lt;- modelResults(logisticModel, testData = test_data, &quot;logisticModel&quot;) The next step is to fit all the classifiers to the data. The code below shows the implemtation of fitting eight models to the credit union data. Note LDA, ANN and Naive Bayes are not included in the parnsip packages, so packages purposely build for applying these alogrithms are used, which also changes the way we collect the performance stats in the next section. The code below shows the model performance analysis used in Chapter 3.3. #--------------------------------------------------------------------------------------- # Output models results using parnsip ouputs (RF, BT, DT, KNN, SVM) randomForestResults &lt;- modelResults(randomForest, testData = test_data, &quot;RandomForest&quot;) boostedTreeResults &lt;- modelResults(boostedTree, testData = test_data, &quot;boostedTree&quot;) decisionTreeResults &lt;- modelResults(decisionTree, testData = test_data, &quot;decisionTree&quot;) KNNTreeResults &lt;- modelResults(KNNmodel, testData = test_data, &quot;KNN&quot;) SVMResults &lt;- modelResults(SVMmodel, testData = test_data, &quot;SVM&quot;) #--------------------------------------------------------------------------------------- # Output models results using LDA pred_raw = predict(LDAModel, test_data, type = &quot;raw&quot;) pred_probs = predict(LDAModel, test_data, type = &quot;prob&quot;) pred_class = predict(LDAModel, test_data, type = &quot;class&quot;) ### Building the predictions of the model and the data with probablity predictions = tibble::tibble(truth = test_data$DEFAULT, predicted = pred_class$class) LDAconfMatrix &lt;- yardstick::conf_mat(predictions, truth, predicted) LDAconfMatrixStats &lt;- summary(LDAconfMatrix) auc &lt;- yardstick::roc_auc(ModelAccuracy, as.factor(truth), class) LDAmodelStats &lt;- rbind(LDAconfMatrixStats, auc) %&gt;% dplyr::mutate(type = &quot;LDA&quot;) #--------------------------------------------------------------------------------------- # Output models results using naive bayes pred_raw = predict(NaiveBayesModel, test_data, type = &quot;raw&quot;) pred_class = predict(NaiveBayesModel, test_data, type = &quot;class&quot;) ## Building the predictions of the model and the data with probablity predictions = tibble::tibble(truth = test_data$DEFAULT, predicted = pred_class) NaiveBayesconfMatrix &lt;- yardstick::conf_mat(predictions, truth, predicted) NaiveBayeconfMatrixStats &lt;- summary(NaiveBayesconfMatrix) NaiveBayeauc &lt;- yardstick::roc_auc(ModelAccuracy, as.factor(truth), class) NaiveBayemodelStats &lt;- rbind(NaiveBayeconfMatrixStats, NaiveBayeauc) %&gt;% dplyr::mutate(type = &quot;NaiveBayes&quot;) #--------------------------------------------------------------------------------------- # Output models results using neurol networks pred_raw = predict(NeuralNetModel.tuned, test_data, type = &quot;raw&quot;) pred_class = predict(NeuralNetModel.tuned, test_data, type = &quot;prob&quot;) class &lt;- as.data.frame(pred_class) %&gt;% dplyr::mutate(class = if_else(`0` &gt; `1`, 0, 1), class = as.factor(class)) %&gt;% dplyr::select(class) colnames(class)[1] &lt;- &quot;class&quot; ## Building the predictions of the model and the data with probablity predictions = tibble::tibble(truth = test_data$DEFAULT, predicted = class$class) ModelAccuracy &lt;- tibble::as_tibble(predictions) %&gt;% dplyr::mutate_if(is.factor, as.numeric) NeuralNetModelconfMatrix &lt;- yardstick::conf_mat(predictions, truth, predicted) NeuralNetModelconfMatrixStats &lt;- summary(NeuralNetModelconfMatrix) NeuralNetModelauc &lt;- yardstick::roc_auc(ModelAccuracy, as.factor(truth), predicted) NeuralNetModelmodelStats &lt;- rbind(NeuralNetModelconfMatrixStats, auc) %&gt;% dplyr::mutate(type = &quot;NeuralNetwork&quot;) Once the performance has been evaluated, four classifiers are taken forward for further analysis. The first step is the re-apply the preprocessing step of up-sampling, as this is implemented in the cross-validation process. This is shows in the code chunk below; The next step is to apply different rebalncing methods to understand which performs the best in terms of predictive performace on the test data. Once the upsampling method is chosen as the best rebalancing method, the best classifiers are put forward for cross validation as shown below. 9.2.3 Chapter 4 The code below shows the model performance analysis used in Chapter 4. #--------------------------------------------------------------------------------------------------- # model performance over resampling methods (k fold - e..g bootstrapping, repeated cv) resamplingmethods &lt;- c(&quot;boot&quot;, &quot;boot632&quot;, &quot;optimism_boot&quot;, &quot;boot_all&quot;, &quot;cv&quot;, &quot;repeatedcv&quot;, &quot;LGOCV&quot;) ModelPerformance &lt;- lapply(X = resamplingmethods, FUN = trainControlSample, data = test_data, numberFolds = 10, samplingMethod = &quot;up&quot;, method = &quot;lda&quot;) %&gt;% data.table:: rbindlist() ModelPerformance &lt;- tidyr:: spread(ModelPerformance, key = Meausure, value = Position) #------------------------------------------------------------------------------- # model performance over rebalancing methods rebalancingMethods &lt;- c(&quot;up&quot;, &quot;down&quot;, &quot;smote&quot;, &quot;rose&quot;) ModelPerformance &lt;- lapply(X = rebalancingMethods, FUN = trainControlSample, method = &quot;lda&quot;, # can apply for others including nb and nnet resamplingmethod = &quot;boot&quot;, #best performing traindata = train_data, testdata = test_data, numberFolds = 10) data = ModelPerformance %&gt;% purrr:: map(1) %&gt;% purrr:: invoke(rbind, .) rebalancePerformance &lt;- tidyr:: spread(data, key = Meausure, value = Position) #------------------------------------------------------------------------------- # model performance over model type modeltypes &lt;- c(&quot;lda&quot;, &quot;glm&quot;, &quot;nb&quot;, &quot;nnet&quot;) ModelPerformance &lt;- lapply(X = modeltypes, FUN = trainControlSample, resamplingmethod = &quot;boot&quot;, #best performing samplingMethod = &quot;up&quot;, #best performing traindata = train_data, testdata = test_data, numberFolds = 5) AllModels &lt;- ModelPerformance %&gt;% purrr:: map(1) %&gt;% purrr:: invoke(rbind, .) %&gt;% tidyr:: spread(key = Meausure, value = Position) ConfusionMatrices &lt;- ModelPerformance %&gt;% purrr:: map(2) #%&gt;% purrr:: invoke(rbind, .) The code below shows steps taken to cross-validate and tune the neural network model library(doParallel) #parallelize code for rfe algorithm cl &lt;- makeCluster(detectCores(), type=&#39;PSOCK&#39;) registerDoParallel(cl) #-------------------------------------------------------------------------------------- fitControl &lt;- trainControl( method = &quot;repeatedcv&quot;, # resampling method number = 5, # k = 5 folds repeats = 5, sampling = &quot;up&quot;) # repeats tune.grid.neuralnet &lt;- expand.grid(size = seq(from = 1, to = 20, by = 2), decay = seq(from = 0.1, to = 1, by = 0.1)) nrow(tune.grid.neuralnet) set.seed(825) nnetFit &lt;- caret:: train(DEFAULT ~ ., data = train_data, method = &quot;nnet&quot;, trControl = fitControl, verbose = FALSE, ## Now specify the different degrees of freedom to apply tuneGrid = tune.grid.neuralnet) 9.2.4 Chapter 5 The code below shows the steps taken to carry out feature importance analysis in Chapter 5. load(file.path(basePath, &quot;./Outputs/nnetFit.RData&quot;)) # compute variable importance importance &lt;- caret::varImp(nnetFit, scale = FALSE) # summarize importance print(importance) data &lt;- tibble::tibble(Importance = round(importance$importance$Overall, 1), Feature = row.names(importance$importance)) colnames(data) &lt;- c(&quot;Importance&quot;, &quot;Feature&quot;) control &lt;- caret::trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 3) model &lt;- caret::train(DEFAULT ~ ., data = train_data, method = &quot;nnet&quot;, trControl = control) imp &lt;- varImp(model) plot(imp) #---------------------------------------------------------------------------------- # define the control using a random forest selection function library(doParallel) #parallelize code for rfe algorithm cl &lt;- makeCluster(detectCores(), type = &quot;PSOCK&quot;) registerDoParallel(cl) control &lt;- caret::rfeControl(functions = rfFuncs, method = &quot;cv&quot;, number = 10) # run the RFE algorithm results &lt;- caret::rfe(DEFAULT ~ ., data = train_data, sizes = c(1:8), rfeControl = control) save(results, file = &quot;./Outputs/RDEresults.RData&quot;) # plot the results ggplot2::ggplot(results, type = c(&quot;g&quot;, &quot;o&quot;)) + ggplot2::geom_line(colour = &quot;#A51140&quot;, size = 1) + ggplot2::theme_minimal() + ggplot2::labs(title = &quot;&quot;, subtitle = &quot;Chart 5: Relative feature importance&quot;, y = &quot;Importance&quot;, x = NULL) + ggplot2::coord_flip() + ggplot2::theme_minimal(base_size = 16) + ggplot2::theme(panel.grid.major.x = element_blank(), panel.border = element_blank(), axis.ticks.x = element_blank(), panel.grid.major = element_blank()) "]
]
